{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS224W_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOewUqkeYT++j/3PxCcM1x0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjiang9/cs224w-project/blob/main/CS224W_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiGBYOrXMBxM"
      },
      "source": [
        "# **GNNs for Detecting Anomalous Users in Wikipedia with Decoupling Representation Learning and Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnYvFlMNTKxV"
      },
      "source": [
        "In this project, our goal is to detect anomalous users in Wikipedia using methods from the paper \"[Decoupling Representation Learning and Classification for GNN-based Anomaly Detection](https://xiaojingzi.github.io/publications/SIGIR21-Wang-et-al-decoupled-GNN.pdf)\". The code and data are borrowed from the paper (source: https://github.com/wyl7/DCI-pytorch). One significant challenge in anomaly detection is the inconsistency between nodes' behavior patterns and label semantics. This paper seeks to alleviate the issue by decoupling the learning of nodes' embeddings from the learning of classifying their labels, rather than learning both in joint training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFOUIiNEVTq6"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZnnFu1kSXlu",
        "outputId": "990c1e26-03d8-41ab-87d7-26bcdd37040d"
      },
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install torch-geometric"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 2.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_sparse-0.6.12-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.19.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.12\n",
            "Collecting torch-geometric\n",
            "  Using cached torch_geometric-2.0.2.tar.gz (325 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.62.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.6.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.1.5)\n",
            "Collecting rdflib\n",
            "  Using cached rdflib-6.0.2-py3-none-any.whl (407 kB)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.6)\n",
            "Collecting yacs\n",
            "  Using cached yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.13)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (57.4.0)\n",
            "Collecting isodate\n",
            "  Using cached isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.2-py3-none-any.whl size=535570 sha256=fc3c76e0fda48dca2cc28a5e21ce2a10710f084e83fdd14625674bedd4a574a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/08/13/2321517088bb2e95bfd0e45033bb9c923189e5b2078e0be4ef\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: isodate, yacs, rdflib, torch-geometric\n",
            "Successfully installed isodate-0.6.0 rdflib-6.0.2 torch-geometric-2.0.2 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFF01qXvUHGU"
      },
      "source": [
        "First, we load and pre-process the dataset, a Wikipedia editor-page graph where nodes are users or Wiki pages, and edges denote the pages that a user has edited. The dataset contains public ground truth labels of banned users - the task is to identify these anomalous users."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQdX-j6CUMWE"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "def load_data(datasets, num_folds):\n",
        "    # load the adjacency\n",
        "    adj = np.loadtxt('./wiki.txt')\n",
        "    num_user = len(set(adj[:, 0]))\n",
        "    num_object = len(set(adj[:, 1]))\n",
        "    adj = adj.astype('int')\n",
        "    nb_nodes = np.max(adj) + 1\n",
        "    edge_index = adj.T\n",
        "    print('Load the edge_index done!')\n",
        "    \n",
        "    # load the user label\n",
        "    label = np.loadtxt('./wiki_label.txt')\n",
        "    y = label[:, 1]\n",
        "    print('Ratio of fraudsters: ', np.sum(y) / len(y))\n",
        "    print('Number of edges: ', edge_index.shape[1])\n",
        "    print('Number of users: ', num_user)\n",
        "    print('Number of objects: ', num_object)\n",
        "    print('Number of nodes: ', nb_nodes)\n",
        "\n",
        "    # split the train_set and validation_set\n",
        "    split_idx = []\n",
        "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=0)\n",
        "    for (train_idx, test_idx) in skf.split(y, y):\n",
        "        split_idx.append((train_idx, test_idx))\n",
        "   \n",
        "    # load initial features\n",
        "    feats = np.load('./wiki_feature64.npy')\n",
        "\n",
        "    return edge_index, feats, split_idx, label, nb_nodes"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s3E7sX-auDM"
      },
      "source": [
        "import numpy as np\n",
        "import sklearn.preprocessing as preprocessing\n",
        "from scipy.sparse import linalg\n",
        "import scipy.sparse as sp\n",
        "import sys\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "def eigen_decomposision(n, k, laplacian, hidden_size, retry):\n",
        "    laplacian = laplacian.astype(\"float64\")\n",
        "    ncv = min(n, max(2 * k + 1, 20))\n",
        "    v0 = np.random.rand(n).astype(\"float64\")\n",
        "    for i in range(retry):\n",
        "        try:\n",
        "            s, u = linalg.eigsh(laplacian, k=k, which=\"LA\", ncv=ncv, v0=v0)\n",
        "           \n",
        "        except sparse.linalg.eigen.arpack.ArpackError:\n",
        "            # print(\"arpack error, retry=\", i)\n",
        "            ncv = min(ncv * 2, n)\n",
        "            if i + 1 == retry:\n",
        "                sparse.save_npz(\"arpack_error_sparse_matrix.npz\", laplacian)\n",
        "                u = torch.zeros(n, k)\n",
        "        else:\n",
        "            break\n",
        "    x = preprocessing.normalize(u, norm=\"l2\")\n",
        "    x = x.astype(\"float64\")\n",
        "    return x\n",
        "\n",
        "def intial_embedding(n, adj, in_degree,hidden_size, retry=10):\n",
        "    in_degree = in_degree.clip(1) ** -0.5\n",
        "    norm = sp.diags(in_degree, 0, dtype=float)\n",
        "    laplacian = norm * adj * norm\n",
        "    k = min(n - 2, hidden_size)\n",
        "    x = eigen_decomposision(n, k, laplacian, hidden_size, retry)\n",
        "    \n",
        "    return x\n",
        "\n",
        "def process_adj(dataSetName):\n",
        "    edges = np.loadtxt(dataSetName).astype('int')\n",
        "    node_num = len(set(edges[:, 0])) + len(set(edges[:, 1]))\n",
        "\n",
        "    row = list(edges[:, 0].T) + list(edges[:, 1].T)\n",
        "    col = list(edges[:, 1].T) + list(edges[:, 0].T)\n",
        "    data = [1.0 for _ in range(len(row))]\n",
        "    adj = sp.csr_matrix((data, (row, col)), shape=(node_num, node_num))\n",
        "    return adj, node_num\n",
        "    \n",
        "adj, n = process_adj('./wiki.txt')\n",
        "hidden_size = 64\n",
        "in_degree = [np.sum(adj.data[adj.indptr[i]: adj.indptr[i+1]]) for i in range(n)]\n",
        "in_degree = np.array(in_degree)\n",
        "x = intial_embedding(n, adj, in_degree, hidden_size, retry=10)\n",
        "np.save('wiki_feature64.npy', x)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBTXDvlOp-8R"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, n_h):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.f_k = nn.Bilinear(n_h, n_h, 1)\n",
        "\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, nn.Bilinear):\n",
        "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, c, h_pl, h_mi, s_bias1=None, s_bias2=None):\n",
        "        c_x = torch.unsqueeze(c, 1)\n",
        "        c_x = c_x.expand_as(h_pl)\n",
        "\n",
        "        sc_1 = torch.squeeze(self.f_k(h_pl, c_x), 2)\n",
        "        sc_2 = torch.squeeze(self.f_k(h_mi, c_x), 2)\n",
        "\n",
        "        if s_bias1 is not None:\n",
        "            sc_1 += s_bias1\n",
        "        if s_bias2 is not None:\n",
        "            sc_2 += s_bias2\n",
        "\n",
        "        logits = torch.cat((sc_1, sc_2), 1)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFZEozHgqKFU"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, num_layers, input_dim, hidden_dim, output_dim):\n",
        "        '''\n",
        "            num_layers: number of layers in the neural networks (EXCLUDING the input layer). If num_layers=1, this reduces to linear model.\n",
        "            input_dim: dimensionality of input features\n",
        "            hidden_dim: dimensionality of hidden units at ALL layers\n",
        "            output_dim: number of classes for prediction\n",
        "            device: which device to use\n",
        "        '''\n",
        "    \n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.linear_or_not = True #default is linear model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        if num_layers < 1:\n",
        "            raise ValueError(\"number of layers should be positive!\")\n",
        "        elif num_layers == 1:\n",
        "            #Linear model\n",
        "            self.linear = nn.Linear(input_dim, output_dim)\n",
        "        else:\n",
        "            #Multi-layer model\n",
        "            self.linear_or_not = False\n",
        "            self.linears = torch.nn.ModuleList()\n",
        "            self.batch_norms = torch.nn.ModuleList()\n",
        "        \n",
        "            self.linears.append(nn.Linear(input_dim, hidden_dim))\n",
        "            for layer in range(num_layers - 2):\n",
        "                self.linears.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            self.linears.append(nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "            for layer in range(num_layers - 1):\n",
        "                self.batch_norms.append(nn.BatchNorm1d((hidden_dim)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.linear_or_not:\n",
        "            #If linear model\n",
        "            return self.linear(x)\n",
        "        else:\n",
        "            #If MLP\n",
        "            h = x\n",
        "            for layer in range(self.num_layers - 1):\n",
        "                h = F.relu(self.batch_norms[layer](self.linears[layer](h)))\n",
        "            return self.linears[self.num_layers - 1](h)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHyWkRU6qL7y"
      },
      "source": [
        "class AvgReadout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AvgReadout, self).__init__()\n",
        "\n",
        "    def forward(self, seq, msk):\n",
        "        if msk is None:\n",
        "            return torch.mean(seq, 1)\n",
        "        else:\n",
        "            msk = torch.unsqueeze(msk, -1)\n",
        "            return torch.sum(seq * msk, 1) / torch.sum(msk)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNjjEvj8SAiq"
      },
      "source": [
        "from torch_geometric.nn.conv import MessagePassing\n",
        "\n",
        "def reset(nn):\n",
        "    def _reset(item):\n",
        "        if hasattr(item, 'reset_parameters'):\n",
        "            item.reset_parameters()\n",
        "\n",
        "    if nn is not None:\n",
        "        if hasattr(nn, 'children') and len(list(nn.children())) > 0:\n",
        "            for item in nn.children():\n",
        "                _reset(item)\n",
        "        else:\n",
        "            _reset(nn)\n",
        "\n",
        "class GINConv(MessagePassing):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        nn (torch.nn.Module): A neural network :math:`h_{\\mathbf{\\Theta}}` that\n",
        "            maps node features :obj:`x` of shape :obj:`[-1, in_channels]` to\n",
        "            shape :obj:`[-1, out_channels]`, *e.g.*, defined by\n",
        "            :class:`torch.nn.Sequential`.\n",
        "        eps (float, optional): (Initial) :math:`\\epsilon`-value.\n",
        "            (default: :obj:`0.`)\n",
        "        train_eps (bool, optional): If set to :obj:`True`, :math:`\\epsilon`\n",
        "            will be a trainable parameter. (default: :obj:`False`)\n",
        "        **kwargs (optional): Additional arguments of\n",
        "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
        "    \"\"\"\n",
        "    def __init__(self, nn, eps: float = 0., train_eps: bool = False,\n",
        "                 **kwargs):\n",
        "        kwargs.setdefault('aggr', 'add')\n",
        "        super().__init__(**kwargs)\n",
        "        self.nn = nn\n",
        "        self.initial_eps = eps\n",
        "        if train_eps:\n",
        "            self.eps = torch.nn.Parameter(torch.Tensor([eps]))\n",
        "        else:\n",
        "            self.register_buffer('eps', torch.Tensor([eps]))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        reset(self.nn)\n",
        "        self.eps.data.fill_(self.initial_eps)\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index) -> torch.Tensor:\n",
        "        \"\"\"\"\"\"\n",
        "        if isinstance(x, torch.Tensor):\n",
        "            x: OptPairTensor = (x, x)\n",
        "\n",
        "        # propagate_type: (x: OptPairTensor)\n",
        "        out = self.propagate(edge_index, x=x)\n",
        "\n",
        "        x_r = x[1]\n",
        "        if x_r is not None:\n",
        "            out += (1 + self.eps) * x_r\n",
        "\n",
        "        return self.nn(out)\n",
        "\n",
        "\n",
        "    def message(self, x_j: torch.Tensor) -> torch.Tensor:\n",
        "        return x_j\n",
        "\n",
        "    def message_and_aggregate(self, adj_t, x) -> torch.Tensor:\n",
        "        adj_t = adj_t.set_value(None, layout=None)\n",
        "        return matmul(adj_t, x[0], reduce=self.aggr)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOtvxBGBZPiF"
      },
      "source": [
        "from torch.nn import Linear, Sequential, ReLU, BatchNorm1d as BN\n",
        "\n",
        "class GIN(torch.nn.Module):\n",
        "    # def __init__(self, dataset, num_layers, hidden):\n",
        "    def __init__(self, num_layers, num_mlp_layers, input_dim, hidden, neighbor_pooling_type, device):\n",
        "        super(GIN, self).__init__()\n",
        "        self.conv1 = GINConv(Sequential(\n",
        "            Linear(input_dim, hidden),\n",
        "            ReLU(),\n",
        "            Linear(hidden, hidden),\n",
        "            ReLU(),\n",
        "            BN(hidden),\n",
        "        ),\n",
        "                             train_eps=True)\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        for i in range(num_layers - 1):\n",
        "            self.convs.append(\n",
        "                GINConv(Sequential(\n",
        "                    Linear(hidden, hidden),\n",
        "                    ReLU(),\n",
        "                    Linear(hidden, hidden),\n",
        "                    ReLU(),\n",
        "                    BN(hidden),\n",
        "                ),\n",
        "                        train_eps=True))\n",
        "        self.lin1 = Linear(hidden, hidden)\n",
        "        self.lin2 = Linear(hidden, hidden)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.conv1.reset_parameters()\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        self.lin1.reset_parameters()\n",
        "        self.lin2.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = self.conv1(x, edge_index)\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index)\n",
        "        return F.log_softmax(x, dim=-1)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXAPxT05qSB_"
      },
      "source": [
        "class GraphCNN(nn.Module):\n",
        "    def __init__(self, num_layers, num_mlp_layers, input_dim, hidden_dim, neighbor_pooling_type, device):\n",
        "        '''\n",
        "            num_layers: number of layers in the neural networks\n",
        "            num_mlp_layers: number of layers in mlps (EXCLUDING the input layer)\n",
        "            input_dim: dimensionality of input features\n",
        "            hidden_dim: dimensionality of hidden units at ALL layers\n",
        "            neighbor_pooling_type: how to aggregate neighbors (mean, average, or max)\n",
        "            device: which device to use\n",
        "        '''\n",
        "\n",
        "        super(GraphCNN, self).__init__()\n",
        "\n",
        "        self.device = device\n",
        "        self.num_layers = num_layers\n",
        "        self.neighbor_pooling_type = neighbor_pooling_type\n",
        "\n",
        "        ###List of MLPs\n",
        "        self.mlps = torch.nn.ModuleList()\n",
        "\n",
        "        ###List of batchnorms applied to the output of MLP (input of the final prediction linear layer)\n",
        "        self.batch_norms = torch.nn.ModuleList()\n",
        "\n",
        "        for layer in range(self.num_layers):\n",
        "            if layer == 0:\n",
        "                self.mlps.append(MLP(num_mlp_layers, input_dim, hidden_dim, hidden_dim))\n",
        "            else:\n",
        "                self.mlps.append(MLP(num_mlp_layers, hidden_dim, hidden_dim, hidden_dim))\n",
        "\n",
        "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
        "\n",
        "    def next_layer(self, h, layer, padded_neighbor_list = None, Adj_block = None):\n",
        "        ###pooling neighboring nodes and center nodes altogether  \n",
        "        \n",
        "        #If sum or average pooling\n",
        "        pooled = torch.spmm(Adj_block, h)\n",
        "        if self.neighbor_pooling_type == \"average\":\n",
        "            #If average pooling\n",
        "            degree = torch.spmm(Adj_block, torch.ones((Adj_block.shape[0], 1)).to(self.device))\n",
        "            \n",
        "            pooled = pooled/degree\n",
        "\n",
        "        #representation of neighboring and center nodes \n",
        "        pooled_rep = self.mlps[layer](pooled)\n",
        "\n",
        "        h = self.batch_norms[layer](pooled_rep)\n",
        "\n",
        "        #non-linearity\n",
        "        h = F.relu(h)\n",
        "        return h\n",
        "\n",
        "    \n",
        "    def forward(self, feats, adj):\n",
        "        h = feats\n",
        "        for layer in range(self.num_layers):\n",
        "            h = self.next_layer(h, layer, Adj_block = adj)\n",
        "\n",
        "        return h"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-wfoTV6pvgf"
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, num_layers, num_mlp_layers, input_dim, hidden_dim, final_dropout, neighbor_pooling_type, device):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.gin = GraphCNN(num_layers, num_mlp_layers, input_dim, hidden_dim, neighbor_pooling_type, device)\n",
        "        # self.gin = GIN(num_layers, num_mlp_layers, input_dim, hidden_dim, neighbor_pooling_type, device)\n",
        "        self.linear_prediction = nn.Linear(hidden_dim, 1)\n",
        "        self.final_dropout = final_dropout\n",
        "        \n",
        "    def forward(self, seq1, adj):\n",
        "        h_1 = self.gin(seq1, adj)\n",
        "        score_final_layer = F.dropout(self.linear_prediction(h_1), \n",
        "                                      self.final_dropout, \n",
        "                                      training = self.training)\n",
        "        return score_final_layer"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRyVHimZqXsy"
      },
      "source": [
        "class DCI(nn.Module):\n",
        "    def __init__(self, num_layers, num_mlp_layers, input_dim, hidden_dim, neighbor_pooling_type, device):\n",
        "        super(DCI, self).__init__()\n",
        "        self.device = device\n",
        "        self.gin = GraphCNN(num_layers, num_mlp_layers, input_dim, hidden_dim, neighbor_pooling_type, device)\n",
        "        # self.gin = GIN(num_layers, num_mlp_layers, input_dim, hidden_dim, neighbor_pooling_type, device)\n",
        "        self.read = AvgReadout()\n",
        "        self.sigm = nn.Sigmoid()\n",
        "        self.disc = Discriminator(hidden_dim)\n",
        "\n",
        "    def forward(self, seq1, seq2, adj, msk, samp_bias1, samp_bias2, cluster_info, cluster_num):\n",
        "        h_1 = self.gin(seq1, adj)\n",
        "        h_2 = self.gin(seq2, adj)\n",
        "\n",
        "        loss = 0\n",
        "        batch_size = 1\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        for i in range(cluster_num):\n",
        "            node_idx = cluster_info[i]\n",
        "\n",
        "            h_1_block = torch.unsqueeze(h_1[node_idx], 0)\n",
        "            c_block = self.read(h_1_block, msk)\n",
        "            c_block = self.sigm(c_block)\n",
        "            h_2_block = torch.unsqueeze(h_2[node_idx], 0)\n",
        "\n",
        "            lbl_1 = torch.ones(batch_size, len(node_idx))\n",
        "            lbl_2 = torch.zeros(batch_size, len(node_idx))\n",
        "            lbl = torch.cat((lbl_1, lbl_2), 1).to(self.device)\n",
        "\n",
        "            ret = self.disc(c_block, h_1_block, h_2_block, samp_bias1, samp_bias2)\n",
        "            loss_tmp = criterion(ret, lbl)\n",
        "            loss += loss_tmp\n",
        "\n",
        "        return loss / cluster_num\n",
        "    \n",
        "    def get_emb(self, seq1, adj):\n",
        "        h_1 = self.gin(seq1, adj)\n",
        "        return h_1"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEOQudnVqccj"
      },
      "source": [
        "class DGI(nn.Module):\n",
        "    def __init__(self, num_layers, num_mlp_layers, input_dim, hidden_dim, neighbor_pooling_type, device):\n",
        "        super(DGI, self).__init__()\n",
        "        self.gin = GraphCNN(num_layers, num_mlp_layers, input_dim, hidden_dim, neighbor_pooling_type, device)\n",
        "        self.read = AvgReadout()\n",
        "        self.sigm = nn.Sigmoid()\n",
        "        self.disc = Discriminator(hidden_dim)\n",
        "\n",
        "    def forward(self, seq1, seq2, adj, msk, samp_bias1, samp_bias2, lbl):\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        h_1 = torch.unsqueeze(self.gin(seq1, adj), 0)\n",
        "\n",
        "        c = self.read(h_1, msk)\n",
        "        c = self.sigm(c)\n",
        "\n",
        "        h_2 = torch.unsqueeze(self.gin(seq2, adj), 0)\n",
        "        \n",
        "        ret = self.disc(c, h_1, h_2, samp_bias1, samp_bias2)\n",
        "\n",
        "        loss = criterion(ret, lbl)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_JjoPVkwswc"
      },
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "sig = torch.nn.Sigmoid()\n",
        "\n",
        "def setup_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True  \n",
        "\n",
        "def preprocess_neighbors_sumavepool(edge_index, nb_nodes, device):\n",
        "    adj_idx = edge_index\n",
        "        \n",
        "    adj_idx_2 = torch.cat([torch.unsqueeze(adj_idx[1], 0), torch.unsqueeze(adj_idx[0], 0)], 0)\n",
        "    adj_idx = torch.cat([adj_idx, adj_idx_2], 1)\n",
        "\n",
        "    self_loop_edge = torch.LongTensor([range(nb_nodes), range(nb_nodes)])\n",
        "    adj_idx = torch.cat([adj_idx, self_loop_edge], 1)\n",
        "        \n",
        "    adj_elem = torch.ones(adj_idx.shape[1])\n",
        "\n",
        "    adj = torch.sparse.FloatTensor(adj_idx, adj_elem, torch.Size([nb_nodes, nb_nodes]))\n",
        "\n",
        "    return adj.to(device)\n",
        "\n",
        "def evaluate(model, test_graph):\n",
        "    output = model(test_graph[0], test_graph[1])\n",
        "    pred = sig(output.detach().cpu())\n",
        "    test_idx = test_graph[3]\n",
        "    \n",
        "    labels = test_graph[-1]\n",
        "    pred = pred[labels[test_idx, 0].astype('int')].numpy()\n",
        "    target = labels[test_idx, 1]\n",
        "    \n",
        "    false_positive_rate, true_positive_rate, _ = metrics.roc_curve(target, pred, pos_label=1)\n",
        "    auc = metrics.auc(false_positive_rate, true_positive_rate)\n",
        "\n",
        "    return auc\n",
        "\n",
        "def finetune(args, model_pretrain, device, test_graph, feats_num):\n",
        "    # initialize the joint model\n",
        "    model = Classifier(args.num_layers, args.num_mlp_layers, feats_num, args.hidden_dim, args.final_dropout, args.neighbor_pooling_type, device).to(device)\n",
        "    \n",
        "    # replace the encoder in joint model with the pre-trained encoder\n",
        "    pretrained_dict = model_pretrain.state_dict()\n",
        "    model_dict = model.state_dict()\n",
        "    pretrained_dict =  {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "    model_dict.update(pretrained_dict)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "    \n",
        "    criterion_tune = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    res = []\n",
        "    train_idx = test_graph[2]\n",
        "    node_train = test_graph[-1][train_idx, 0].astype('int')\n",
        "    label_train = torch.FloatTensor(test_graph[-1][train_idx, 1]).to(device)\n",
        "    for _ in range(1, args.finetune_epochs+1):\n",
        "        model.train()\n",
        "        output = model(test_graph[0], test_graph[1])\n",
        "        loss = criterion_tune(output[node_train], torch.reshape(label_train, (-1, 1)))\n",
        "        \n",
        "        #backprop\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        # testing\n",
        "        model.eval()\n",
        "        auc = evaluate(model, test_graph)\n",
        "        res.append(auc)\n",
        "\n",
        "    return np.max(res)\n",
        "\n",
        "def main_dci():\n",
        "    parser = argparse.ArgumentParser(description='PyTorch deep cluster infomax')\n",
        "    parser.add_argument('--dataset', type=str, default=\"wiki\",\n",
        "                        help='name of dataset (default: wiki)')\n",
        "    parser.add_argument('--device', type=int, default=0,\n",
        "                        help='which gpu to use if any (default: 0)')\n",
        "    parser.add_argument('--epochs', type=int, default=50,\n",
        "                        help='number of epochs to train (default: 50)')\n",
        "    parser.add_argument('--num_layers', type=int, default=2,\n",
        "                        help='number of layers (default: 2)')\n",
        "    parser.add_argument('--num_mlp_layers', type=int, default=2,\n",
        "                        help='number of layers for MLP EXCLUDING the input one (default: 2). 1 means linear model.')\n",
        "    parser.add_argument('--hidden_dim', type=int, default=128,\n",
        "                        help='number of hidden units (default: 128)')\n",
        "    parser.add_argument('--finetune_epochs', type=int, default=100,\n",
        "                        help='number of finetune epochs (default: 100)')\n",
        "    parser.add_argument('--num_folds', type=int, default=10,\n",
        "                        help='number of folds (default: 10)')\n",
        "    parser.add_argument('--lr', type=float, default=0.01,\n",
        "                        help='learning rate (default: 0.01)')\n",
        "    parser.add_argument('--num_cluster', type=int, default=2,\n",
        "                        help='number of clusters (default: 2)')\n",
        "    parser.add_argument('--recluster_interval', type=int, default=20,\n",
        "                        help='the interval of reclustering (default: 20)')\n",
        "    parser.add_argument('--final_dropout', type=float, default=0.5,\n",
        "                        help='final layer dropout (default: 0.5)')\n",
        "    parser.add_argument('--neighbor_pooling_type', type=str, default=\"sum\", choices=[\"sum\", \"average\"],\n",
        "                        help='Pooling for over neighboring nodes: sum or average')\n",
        "    parser.add_argument('--training_scheme', type=str, default=\"decoupled\", choices=[\"decoupled\", \"joint\"],\n",
        "                        help='Training schemes: decoupled or joint')\n",
        "    args = parser.parse_args(args={})\n",
        "\n",
        "    setup_seed(0)\n",
        "    \n",
        "    device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "    # data loading\n",
        "    edge_index, feats, split_idx, label, nb_nodes = load_data(args.dataset, args.num_folds)\n",
        "    input_dim = feats.shape[1]\n",
        "    # pre-clustering and store userID in each clusters\n",
        "    kmeans = KMeans(n_clusters=args.num_cluster, random_state=0).fit(feats)\n",
        "    ss_label = kmeans.labels_\n",
        "    cluster_info = [list(np.where(ss_label==i)[0]) for i in range(args.num_cluster)]\n",
        "    # the shuffled features are used to contruct the negative sample-pairs\n",
        "    idx = np.random.permutation(nb_nodes)\n",
        "    shuf_feats = feats[idx, :]\n",
        "\n",
        "    adj = preprocess_neighbors_sumavepool(torch.LongTensor(edge_index), nb_nodes, device)\n",
        "    feats = torch.FloatTensor(feats).to(device)\n",
        "    shuf_feats = torch.FloatTensor(shuf_feats).to(device)\n",
        "\n",
        "    # pre-training process\n",
        "    model_pretrain = DCI(args.num_layers, args.num_mlp_layers, input_dim, args.hidden_dim, args.neighbor_pooling_type, device).to(device)\n",
        "    if args.training_scheme == 'decoupled':\n",
        "        optimizer_train = optim.Adam(model_pretrain.parameters(), lr=args.lr)\n",
        "        for epoch in range(1, args.epochs + 1):\n",
        "            model_pretrain.train()\n",
        "            loss_pretrain = model_pretrain(feats, shuf_feats, adj, None, None, None, cluster_info, args.num_cluster)\n",
        "            if optimizer_train is not None:\n",
        "                optimizer_train.zero_grad()\n",
        "                loss_pretrain.backward()         \n",
        "                optimizer_train.step()\n",
        "            # re-clustering\n",
        "            if epoch % args.recluster_interval == 0 and epoch < args.epochs:\n",
        "                model_pretrain.eval()\n",
        "                emb = model_pretrain.get_emb(feats, adj)\n",
        "                kmeans = KMeans(n_clusters=args.num_cluster, random_state=0).fit(emb.detach().cpu().numpy())\n",
        "                ss_label = kmeans.labels_\n",
        "                cluster_info = [list(np.where(ss_label==i)[0]) for i in range(args.num_cluster)]\n",
        "        \n",
        "        print('Pre-training Down!')\n",
        "            \n",
        "    #fine-tuning process\n",
        "    fold_idx = 1\n",
        "    every_fold_auc = []\n",
        "    for (train_idx, test_idx) in split_idx:\n",
        "        test_graph = (feats, adj, train_idx, test_idx, label)\n",
        "        tmp_auc = finetune(args, model_pretrain, device, test_graph, input_dim)\n",
        "        every_fold_auc.append(tmp_auc)\n",
        "        print('AUC on the Fold'+str(fold_idx)+': ', tmp_auc)\n",
        "        fold_idx += 1\n",
        "    print('The averaged AUC score: ', np.mean(every_fold_auc))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIKQn7MTxI_d",
        "outputId": "f7f112dc-e7b3-46f7-8c7b-7fe29b3e8983"
      },
      "source": [
        "main_dci()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load the edge_index done!\n",
            "Ratio of fraudsters:  0.026376564969004496\n",
            "Number of edges:  18257\n",
            "Number of users:  8227\n",
            "Number of objects:  1000\n",
            "Number of nodes:  9227\n",
            "Pre-training Down!\n",
            "AUC on the Fold1:  0.6929122687549656\n",
            "AUC on the Fold2:  0.7587107025309273\n",
            "AUC on the Fold3:  0.7269889910339349\n",
            "AUC on the Fold4:  0.8275167404380888\n",
            "AUC on the Fold5:  0.7306775621382362\n",
            "AUC on the Fold6:  0.6915219611848826\n",
            "AUC on the Fold7:  0.7615196912949721\n",
            "AUC on the Fold8:  0.6705903335116818\n",
            "AUC on the Fold9:  0.6252600915522263\n",
            "AUC on the Fold10:  0.7732596159562451\n",
            "The averaged AUC score:  0.7258957958396162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "PxpEq3j9aNWN",
        "outputId": "cad6dffe-e50e-4574-8c25-18e4befb319c"
      },
      "source": [
        "main_dci()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load the edge_index done!\n",
            "Ratio of fraudsters:  0.026376564969004496\n",
            "Number of edges:  18257\n",
            "Number of users:  8227\n",
            "Number of objects:  1000\n",
            "Number of nodes:  9227\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-cb7f33d9093d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain_dci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-041b6c659c9e>\u001b[0m in \u001b[0;36mmain_dci\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mmodel_pretrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mloss_pretrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_pretrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuf_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_cluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moptimizer_train\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0moptimizer_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-3640b0c501fb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, seq1, seq2, adj, msk, samp_bias1, samp_bias2, cluster_info, cluster_num)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamp_bias1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamp_bias2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mh_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mh_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-2c00c24f4cdc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# x, edge_index, batch = data.x, data.edge_index, data.batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-4cb1f129dbcd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# propagate_type: (x: OptPairTensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mx_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__check_input__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;31m# Run \"fused\" message and aggregation (if applicable).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36m__check_input__\u001b[0;34m(self, edge_index, size)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEupCHyd1WSo"
      },
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import random\n",
        "\n",
        "sig = torch.nn.Sigmoid()\n",
        "\n",
        "def setup_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True  \n",
        "\n",
        "def preprocess_neighbors_sumavepool(edge_index, nb_nodes, device):\n",
        "    adj_idx = edge_index\n",
        "        \n",
        "    adj_idx_2 = torch.cat([torch.unsqueeze(adj_idx[1], 0), torch.unsqueeze(adj_idx[0], 0)], 0)\n",
        "    adj_idx = torch.cat([adj_idx, adj_idx_2], 1)\n",
        "\n",
        "    self_loop_edge = torch.LongTensor([range(nb_nodes), range(nb_nodes)])\n",
        "    adj_idx = torch.cat([adj_idx, self_loop_edge], 1)\n",
        "        \n",
        "    adj_elem = torch.ones(adj_idx.shape[1])\n",
        "\n",
        "    adj = torch.sparse.FloatTensor(adj_idx, adj_elem, torch.Size([nb_nodes, nb_nodes]))\n",
        "\n",
        "    return adj.to(device)\n",
        "\n",
        "def evaluate(model, test_graph):\n",
        "    output = model(test_graph[0], test_graph[1])\n",
        "    pred = sig(output.detach().cpu())\n",
        "    test_idx = test_graph[3]\n",
        "    \n",
        "    labels = test_graph[-1]\n",
        "    pred = pred[labels[test_idx, 0].astype('int')].numpy()\n",
        "    target = labels[test_idx, 1]\n",
        "    \n",
        "    false_positive_rate, true_positive_rate, _ = metrics.roc_curve(target, pred, pos_label=1)\n",
        "    auc = metrics.auc(false_positive_rate, true_positive_rate)\n",
        "\n",
        "    return auc\n",
        "\n",
        "def finetune(args, model_pretrain, device, test_graph, feats_num):\n",
        "    # initialize the joint model\n",
        "    model = Classifier(args.num_layers, args.num_mlp_layers, feats_num, args.hidden_dim, args.final_dropout, args.neighbor_pooling_type, device).to(device)\n",
        "    \n",
        "    # replace the encoder in joint model with the pre-trained encoder\n",
        "    pretrained_dict = model_pretrain.state_dict()\n",
        "    model_dict = model.state_dict()\n",
        "    pretrained_dict =  {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "    model_dict.update(pretrained_dict)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "    \n",
        "    criterion_tune = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    res = []\n",
        "    train_idx = test_graph[2]\n",
        "    node_train = test_graph[-1][train_idx, 0].astype('int')\n",
        "    label_train = torch.FloatTensor(test_graph[-1][train_idx, 1]).to(device)\n",
        "    for _ in range(1, args.finetune_epochs+1):\n",
        "        model.train()\n",
        "        output = model(test_graph[0], test_graph[1])\n",
        "        loss = criterion_tune(output[node_train], torch.reshape(label_train, (-1, 1)))\n",
        "        \n",
        "        #backprop\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        # testing\n",
        "        model.eval()\n",
        "        auc = evaluate(model, test_graph)\n",
        "        res.append(auc)\n",
        "\n",
        "    return np.max(res)\n",
        "\n",
        "\n",
        "def main_dgi():\n",
        "    parser = argparse.ArgumentParser(description='PyTorch graph convolutional neurasl net')\n",
        "    parser.add_argument('--dataset', type=str, default=\"wiki\",\n",
        "                        help='name of dataset (default: wiki)')\n",
        "    parser.add_argument('--device', type=int, default=0,\n",
        "                        help='which gpu to use if any (default: 0)')\n",
        "    parser.add_argument('--epochs', type=int, default=50,\n",
        "                        help='number of epochs to train (default: 50)')\n",
        "    parser.add_argument('--finetune_epochs', type=int, default=100,\n",
        "                        help='number of finetune epochs (default: 100)')\n",
        "    parser.add_argument('--num_layers', type=int, default=2,\n",
        "                        help='number of layers (default: 2)')\n",
        "    parser.add_argument('--num_mlp_layers', type=int, default=2,\n",
        "                        help='number of layers for MLP EXCLUDING the input one (default: 2). 1 means linear model.')\n",
        "    parser.add_argument('--hidden_dim', type=int, default=128,\n",
        "                        help='number of hidden units (default: 128)')\n",
        "    parser.add_argument('--num_folds', type=int, default=10,\n",
        "                        help='number of folds (default: 10)')\n",
        "    parser.add_argument('--lr', type=float, default=0.01,\n",
        "                        help='learning rate (default: 0.01)')\n",
        "    parser.add_argument('--final_dropout', type=float, default=0.5,\n",
        "                        help='final layer dropout (default: 0.5)')\n",
        "    parser.add_argument('--neighbor_pooling_type', type=str, default=\"sum\", choices=[\"sum\", \"average\"],\n",
        "                        help='Pooling for over neighboring nodes: sum or average')\n",
        "    args = parser.parse_args(args={})\n",
        "\n",
        "    setup_seed(0)\n",
        "    \n",
        "    device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    # Data loading\n",
        "    edge_index, feats, split_idx, label, nb_nodes = load_data(args.dataset, args.num_folds)\n",
        "    input_dim = feats.shape[1]\n",
        "    # the shuffled features are used to contruct the negative samples\n",
        "    idx = np.random.permutation(nb_nodes)\n",
        "    shuf_feats = feats[idx, :]\n",
        "\n",
        "    model_pretrain = DGI(args.num_layers, args.num_mlp_layers, input_dim, args.hidden_dim, args.neighbor_pooling_type, device).to(device)\n",
        "    optimizer_train = optim.Adam(model_pretrain.parameters(), lr=args.lr)\n",
        "\n",
        "    batch_size = 1\n",
        "    lbl_1 = torch.ones(batch_size, nb_nodes)\n",
        "    lbl_2 = torch.zeros(batch_size, nb_nodes)\n",
        "    lbl = torch.cat((lbl_1, lbl_2), 1).to(device)\n",
        "\n",
        "    adj = preprocess_neighbors_sumavepool(torch.LongTensor(edge_index), nb_nodes, device)\n",
        "    feats = torch.FloatTensor(feats).to(device)\n",
        "    shuf_feats = torch.FloatTensor(shuf_feats).to(device)\n",
        "    \n",
        "    # pre-training\n",
        "    model_pretrain.train()\n",
        "    for _ in range(1, args.epochs + 1):\n",
        "        loss_pretrain = model_pretrain(feats, shuf_feats, adj, None, None, None, lbl)\n",
        "        if optimizer_train is not None:\n",
        "            optimizer_train.zero_grad()\n",
        "            loss_pretrain.backward()         \n",
        "            optimizer_train.step()\n",
        "    \n",
        "    print('Pre-training done!')\n",
        "\n",
        "    #fine-tuning process\n",
        "    fold_idx = 1\n",
        "    every_fold_auc = []\n",
        "    for (train_idx, test_idx) in split_idx:\n",
        "        test_graph = (feats, adj, train_idx, test_idx, label)\n",
        "        tmp_auc = finetune(args, model_pretrain, device, test_graph, input_dim)\n",
        "        every_fold_auc.append(tmp_auc)\n",
        "        print('AUC on the Fold'+str(fold_idx)+': ', tmp_auc)\n",
        "        fold_idx += 1\n",
        "    \n",
        "    print('The averaged AUC score: ', np.mean(every_fold_auc))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lvHmlgd2USV",
        "outputId": "8cec6bfd-cc2b-4558-fea6-dec1a78be315"
      },
      "source": [
        "main_dgi()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load the edge_index done!\n",
            "Ratio of fraudsters:  0.026376564969004496\n",
            "Number of edges:  18257\n",
            "Number of users:  8227\n",
            "Number of objects:  1000\n",
            "Number of nodes:  9227\n",
            "Pre-training done!\n",
            "AUC on the Fold1:  0.7252581999773011\n",
            "AUC on the Fold2:  0.7650096470321188\n",
            "AUC on the Fold3:  0.7708546135512429\n",
            "AUC on the Fold4:  0.7839348541595733\n",
            "AUC on the Fold5:  0.7314152763590966\n",
            "AUC on the Fold6:  0.7459993190330269\n",
            "AUC on the Fold7:  0.755163999546022\n",
            "AUC on the Fold8:  0.7167231436894359\n",
            "AUC on the Fold9:  0.6118839545805838\n",
            "AUC on the Fold10:  0.7520361452945723\n",
            "The averaged AUC score:  0.7358279153222974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJV-u_Iy2VUx"
      },
      "source": [
        ""
      ],
      "execution_count": 26,
      "outputs": []
    }
  ]
}