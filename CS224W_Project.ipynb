{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS224W_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNIEq1yjO3MMDsaxWFgxzbo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjiang9/cs224w-project/blob/main/CS224W_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFOUIiNEVTq6"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBTXDvlOp-8R"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, n_h):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.f_k = nn.Bilinear(n_h, n_h, 1)\n",
        "\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, nn.Bilinear):\n",
        "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, c, h_pl, h_mi, s_bias1=None, s_bias2=None):\n",
        "        c_x = torch.unsqueeze(c, 1)\n",
        "        c_x = c_x.expand_as(h_pl)\n",
        "\n",
        "        sc_1 = torch.squeeze(self.f_k(h_pl, c_x), 2)\n",
        "        sc_2 = torch.squeeze(self.f_k(h_mi, c_x), 2)\n",
        "\n",
        "        if s_bias1 is not None:\n",
        "            sc_1 += s_bias1\n",
        "        if s_bias2 is not None:\n",
        "            sc_2 += s_bias2\n",
        "\n",
        "        logits = torch.cat((sc_1, sc_2), 1)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFZEozHgqKFU"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, num_layers, input_dim, hidden_dim, output_dim):\n",
        "        '''\n",
        "            num_layers: number of layers in the neural networks (EXCLUDING the input layer). If num_layers=1, this reduces to linear model.\n",
        "            input_dim: dimensionality of input features\n",
        "            hidden_dim: dimensionality of hidden units at ALL layers\n",
        "            output_dim: number of classes for prediction\n",
        "            device: which device to use\n",
        "        '''\n",
        "    \n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.linear_or_not = True #default is linear model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        if num_layers < 1:\n",
        "            raise ValueError(\"number of layers should be positive!\")\n",
        "        elif num_layers == 1:\n",
        "            #Linear model\n",
        "            self.linear = nn.Linear(input_dim, output_dim)\n",
        "        else:\n",
        "            #Multi-layer model\n",
        "            self.linear_or_not = False\n",
        "            self.linears = torch.nn.ModuleList()\n",
        "            self.batch_norms = torch.nn.ModuleList()\n",
        "        \n",
        "            self.linears.append(nn.Linear(input_dim, hidden_dim))\n",
        "            for layer in range(num_layers - 2):\n",
        "                self.linears.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            self.linears.append(nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "            for layer in range(num_layers - 1):\n",
        "                self.batch_norms.append(nn.BatchNorm1d((hidden_dim)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.linear_or_not:\n",
        "            #If linear model\n",
        "            return self.linear(x)\n",
        "        else:\n",
        "            #If MLP\n",
        "            h = x\n",
        "            for layer in range(self.num_layers - 1):\n",
        "                h = F.relu(self.batch_norms[layer](self.linears[layer](h)))\n",
        "            return self.linears[self.num_layers - 1](h)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHyWkRU6qL7y"
      },
      "source": [
        "class AvgReadout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AvgReadout, self).__init__()\n",
        "\n",
        "    def forward(self, seq, msk):\n",
        "        if msk is None:\n",
        "            return torch.mean(seq, 1)\n",
        "        else:\n",
        "            msk = torch.unsqueeze(msk, -1)\n",
        "            return torch.sum(seq * msk, 1) / torch.sum(msk)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXAPxT05qSB_"
      },
      "source": [
        "class GraphCNN(nn.Module):\n",
        "    def __init__(self, num_layers, num_mlp_layers, input_dim, hidden_dim, neighbor_pooling_type, device):\n",
        "        '''\n",
        "            num_layers: number of layers in the neural networks\n",
        "            num_mlp_layers: number of layers in mlps (EXCLUDING the input layer)\n",
        "            input_dim: dimensionality of input features\n",
        "            hidden_dim: dimensionality of hidden units at ALL layers\n",
        "            neighbor_pooling_type: how to aggregate neighbors (mean, average, or max)\n",
        "            device: which device to use\n",
        "        '''\n",
        "\n",
        "        super(GraphCNN, self).__init__()\n",
        "\n",
        "        self.device = device\n",
        "        self.num_layers = num_layers\n",
        "        self.neighbor_pooling_type = neighbor_pooling_type\n",
        "\n",
        "        ###List of MLPs\n",
        "        self.mlps = torch.nn.ModuleList()\n",
        "\n",
        "        ###List of batchnorms applied to the output of MLP (input of the final prediction linear layer)\n",
        "        self.batch_norms = torch.nn.ModuleList()\n",
        "\n",
        "        for layer in range(self.num_layers):\n",
        "            if layer == 0:\n",
        "                self.mlps.append(MLP(num_mlp_layers, input_dim, hidden_dim, hidden_dim))\n",
        "            else:\n",
        "                self.mlps.append(MLP(num_mlp_layers, hidden_dim, hidden_dim, hidden_dim))\n",
        "\n",
        "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
        "\n",
        "    def next_layer(self, h, layer, padded_neighbor_list = None, Adj_block = None):\n",
        "        ###pooling neighboring nodes and center nodes altogether  \n",
        "        \n",
        "        #If sum or average pooling\n",
        "        pooled = torch.spmm(Adj_block, h)\n",
        "        if self.neighbor_pooling_type == \"average\":\n",
        "            #If average pooling\n",
        "            degree = torch.spmm(Adj_block, torch.ones((Adj_block.shape[0], 1)).to(self.device))\n",
        "            \n",
        "            pooled = pooled/degree\n",
        "\n",
        "        #representation of neighboring and center nodes \n",
        "        pooled_rep = self.mlps[layer](pooled)\n",
        "\n",
        "        h = self.batch_norms[layer](pooled_rep)\n",
        "\n",
        "        #non-linearity\n",
        "        h = F.relu(h)\n",
        "        return h\n",
        "\n",
        "    \n",
        "    def forward(self, feats, adj):\n",
        "        h = feats\n",
        "        for layer in range(self.num_layers):\n",
        "            h = self.next_layer(h, layer, Adj_block = adj)\n",
        "\n",
        "        return h"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-wfoTV6pvgf"
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, num_layers, num_mlp_layers, input_dim, hidden_dim, final_dropout, neighbor_pooling_type, device):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.gin = GraphCNN(num_layers, num_mlp_layers, input_dim, hidden_dim, neighbor_pooling_type, device)\n",
        "        self.linear_prediction = nn.Linear(hidden_dim, 1)\n",
        "        self.final_dropout = final_dropout\n",
        "        \n",
        "    def forward(self, seq1, adj):\n",
        "        h_1 = self.gin(seq1, adj)\n",
        "        score_final_layer = F.dropout(self.linear_prediction(h_1), self.final_dropout, training = self.training)\n",
        "        return score_final_layer"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRyVHimZqXsy"
      },
      "source": [
        "class DCI(nn.Module):\n",
        "    def __init__(self, num_layers, num_mlp_layers, input_dim, hidden_dim, neighbor_pooling_type, device):\n",
        "        super(DCI, self).__init__()\n",
        "        self.device = device\n",
        "        self.gin = GraphCNN(num_layers, num_mlp_layers, input_dim, hidden_dim, neighbor_pooling_type, device)\n",
        "        self.read = AvgReadout()\n",
        "        self.sigm = nn.Sigmoid()\n",
        "        self.disc = Discriminator(hidden_dim)\n",
        "\n",
        "    def forward(self, seq1, seq2, adj, msk, samp_bias1, samp_bias2, cluster_info, cluster_num):\n",
        "        h_1 = self.gin(seq1, adj)\n",
        "        h_2 = self.gin(seq2, adj)\n",
        "\n",
        "        loss = 0\n",
        "        batch_size = 1\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        for i in range(cluster_num):\n",
        "            node_idx = cluster_info[i]\n",
        "\n",
        "            h_1_block = torch.unsqueeze(h_1[node_idx], 0)\n",
        "            c_block = self.read(h_1_block, msk)\n",
        "            c_block = self.sigm(c_block)\n",
        "            h_2_block = torch.unsqueeze(h_2[node_idx], 0)\n",
        "\n",
        "            lbl_1 = torch.ones(batch_size, len(node_idx))\n",
        "            lbl_2 = torch.zeros(batch_size, len(node_idx))\n",
        "            lbl = torch.cat((lbl_1, lbl_2), 1).to(self.device)\n",
        "\n",
        "            ret = self.disc(c_block, h_1_block, h_2_block, samp_bias1, samp_bias2)\n",
        "            loss_tmp = criterion(ret, lbl)\n",
        "            loss += loss_tmp\n",
        "\n",
        "        return loss / cluster_num\n",
        "    \n",
        "    def get_emb(self, seq1, adj):\n",
        "        h_1 = self.gin(seq1, adj)\n",
        "        return h_1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEOQudnVqccj"
      },
      "source": [
        "class DGI(nn.Module):\n",
        "    def __init__(self, num_layers, num_mlp_layers, input_dim, hidden_dim, neighbor_pooling_type, device):\n",
        "        super(DGI, self).__init__()\n",
        "        self.gin = GraphCNN(num_layers, num_mlp_layers, input_dim, hidden_dim, neighbor_pooling_type, device)\n",
        "        self.read = AvgReadout()\n",
        "        self.sigm = nn.Sigmoid()\n",
        "        self.disc = Discriminator(hidden_dim)\n",
        "\n",
        "    def forward(self, seq1, seq2, adj, msk, samp_bias1, samp_bias2, lbl):\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        h_1 = torch.unsqueeze(self.gin(seq1, adj), 0)\n",
        "\n",
        "        c = self.read(h_1, msk)\n",
        "        c = self.sigm(c)\n",
        "\n",
        "        h_2 = torch.unsqueeze(self.gin(seq2, adj), 0)\n",
        "        \n",
        "        ret = self.disc(c, h_1, h_2, samp_bias1, samp_bias2)\n",
        "\n",
        "        loss = criterion(ret, lbl)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rvs1_Y92qfH7"
      },
      "source": [
        "import numpy as np\n",
        "import sklearn.preprocessing as preprocessing\n",
        "from scipy.sparse import linalg\n",
        "import scipy.sparse as sp\n",
        "import sys\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "def eigen_decomposision(n, k, laplacian, hidden_size, retry):\n",
        "    laplacian = laplacian.astype(\"float64\")\n",
        "    ncv = min(n, max(2 * k + 1, 20))\n",
        "    v0 = np.random.rand(n).astype(\"float64\")\n",
        "    for i in range(retry):\n",
        "        try:\n",
        "            s, u = linalg.eigsh(laplacian, k=k, which=\"LA\", ncv=ncv, v0=v0)\n",
        "           \n",
        "        except sparse.linalg.eigen.arpack.ArpackError:\n",
        "            # print(\"arpack error, retry=\", i)\n",
        "            ncv = min(ncv * 2, n)\n",
        "            if i + 1 == retry:\n",
        "                sparse.save_npz(\"arpack_error_sparse_matrix.npz\", laplacian)\n",
        "                u = torch.zeros(n, k)\n",
        "        else:\n",
        "            break\n",
        "    x = preprocessing.normalize(u, norm=\"l2\")\n",
        "    x = x.astype(\"float64\")\n",
        "    return x\n",
        "\n",
        "def intial_embedding(n, adj, in_degree,hidden_size, retry=10):\n",
        "    in_degree = in_degree.clip(1) ** -0.5\n",
        "    norm = sp.diags(in_degree, 0, dtype=float)\n",
        "    laplacian = norm * adj * norm\n",
        "    k = min(n - 2, hidden_size)\n",
        "    x = eigen_decomposision(n, k, laplacian, hidden_size, retry)\n",
        "    \n",
        "    return x\n",
        "\n",
        "def process_adj(dataSetName):\n",
        "    edges = np.loadtxt(dataSetName).astype('int')\n",
        "    node_num = len(set(edges[:, 0])) + len(set(edges[:, 1]))\n",
        "\n",
        "    row = list(edges[:, 0].T) + list(edges[:, 1].T)\n",
        "    col = list(edges[:, 1].T) + list(edges[:, 0].T)\n",
        "    data = [1.0 for _ in range(len(row))]\n",
        "    adj = sp.csr_matrix((data, (row, col)), shape=(node_num, node_num))\n",
        "    return adj, node_num"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf0tu0lnqi9N"
      },
      "source": [
        "adj, n = process_adj('./wiki.txt')\n",
        "hidden_size = 64\n",
        "in_degree = [np.sum(adj.data[adj.indptr[i]: adj.indptr[i+1]]) for i in range(n)]\n",
        "in_degree = np.array(in_degree)\n",
        "x = intial_embedding(n, adj, in_degree, hidden_size, retry=10)\n",
        "np.save('wiki_feature64.npy', x)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxCBklM5uanV"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "def load_data(datasets, num_folds):\n",
        "    # load the adjacency\n",
        "    adj = np.loadtxt('./wiki.txt')\n",
        "    num_user = len(set(adj[:, 0]))\n",
        "    num_object = len(set(adj[:, 1]))\n",
        "    adj = adj.astype('int')\n",
        "    nb_nodes = np.max(adj) + 1\n",
        "    edge_index = adj.T\n",
        "    print('Load the edge_index done!')\n",
        "    \n",
        "    # load the user label\n",
        "    label = np.loadtxt('./wiki_label.txt')\n",
        "    y = label[:, 1]\n",
        "    print('Ratio of fraudsters: ', np.sum(y) / len(y))\n",
        "    print('Number of edges: ', edge_index.shape[1])\n",
        "    print('Number of users: ', num_user)\n",
        "    print('Number of objects: ', num_object)\n",
        "    print('Number of nodes: ', nb_nodes)\n",
        "\n",
        "    # split the train_set and validation_set\n",
        "    split_idx = []\n",
        "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=0)\n",
        "    for (train_idx, test_idx) in skf.split(y, y):\n",
        "        split_idx.append((train_idx, test_idx))\n",
        "   \n",
        "    # load initial features\n",
        "    feats = np.load('./wiki_feature64.npy')\n",
        "\n",
        "    return edge_index, feats, split_idx, label, nb_nodes"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_JjoPVkwswc"
      },
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "sig = torch.nn.Sigmoid()\n",
        "\n",
        "def setup_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True  \n",
        "\n",
        "def preprocess_neighbors_sumavepool(edge_index, nb_nodes, device):\n",
        "    adj_idx = edge_index\n",
        "        \n",
        "    adj_idx_2 = torch.cat([torch.unsqueeze(adj_idx[1], 0), torch.unsqueeze(adj_idx[0], 0)], 0)\n",
        "    adj_idx = torch.cat([adj_idx, adj_idx_2], 1)\n",
        "\n",
        "    self_loop_edge = torch.LongTensor([range(nb_nodes), range(nb_nodes)])\n",
        "    adj_idx = torch.cat([adj_idx, self_loop_edge], 1)\n",
        "        \n",
        "    adj_elem = torch.ones(adj_idx.shape[1])\n",
        "\n",
        "    adj = torch.sparse.FloatTensor(adj_idx, adj_elem, torch.Size([nb_nodes, nb_nodes]))\n",
        "\n",
        "    return adj.to(device)\n",
        "\n",
        "def evaluate(model, test_graph):\n",
        "    output = model(test_graph[0], test_graph[1])\n",
        "    pred = sig(output.detach().cpu())\n",
        "    test_idx = test_graph[3]\n",
        "    \n",
        "    labels = test_graph[-1]\n",
        "    pred = pred[labels[test_idx, 0].astype('int')].numpy()\n",
        "    target = labels[test_idx, 1]\n",
        "    \n",
        "    false_positive_rate, true_positive_rate, _ = metrics.roc_curve(target, pred, pos_label=1)\n",
        "    auc = metrics.auc(false_positive_rate, true_positive_rate)\n",
        "\n",
        "    return auc\n",
        "\n",
        "def finetune(args, model_pretrain, device, test_graph, feats_num):\n",
        "    # initialize the joint model\n",
        "    model = Classifier(args.num_layers, args.num_mlp_layers, feats_num, args.hidden_dim, args.final_dropout, args.neighbor_pooling_type, device).to(device)\n",
        "    \n",
        "    # replace the encoder in joint model with the pre-trained encoder\n",
        "    pretrained_dict = model_pretrain.state_dict()\n",
        "    model_dict = model.state_dict()\n",
        "    pretrained_dict =  {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "    model_dict.update(pretrained_dict)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "    \n",
        "    criterion_tune = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    res = []\n",
        "    train_idx = test_graph[2]\n",
        "    node_train = test_graph[-1][train_idx, 0].astype('int')\n",
        "    label_train = torch.FloatTensor(test_graph[-1][train_idx, 1]).to(device)\n",
        "    for _ in range(1, args.finetune_epochs+1):\n",
        "        model.train()\n",
        "        output = model(test_graph[0], test_graph[1])\n",
        "        loss = criterion_tune(output[node_train], torch.reshape(label_train, (-1, 1)))\n",
        "        \n",
        "        #backprop\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        # testing\n",
        "        model.eval()\n",
        "        auc = evaluate(model, test_graph)\n",
        "        res.append(auc)\n",
        "\n",
        "    return np.max(res)\n",
        "\n",
        "def main_dci():\n",
        "    parser = argparse.ArgumentParser(description='PyTorch deep cluster infomax')\n",
        "    parser.add_argument('--dataset', type=str, default=\"wiki\",\n",
        "                        help='name of dataset (default: wiki)')\n",
        "    parser.add_argument('--device', type=int, default=0,\n",
        "                        help='which gpu to use if any (default: 0)')\n",
        "    parser.add_argument('--epochs', type=int, default=50,\n",
        "                        help='number of epochs to train (default: 50)')\n",
        "    parser.add_argument('--num_layers', type=int, default=2,\n",
        "                        help='number of layers (default: 2)')\n",
        "    parser.add_argument('--num_mlp_layers', type=int, default=2,\n",
        "                        help='number of layers for MLP EXCLUDING the input one (default: 2). 1 means linear model.')\n",
        "    parser.add_argument('--hidden_dim', type=int, default=128,\n",
        "                        help='number of hidden units (default: 128)')\n",
        "    parser.add_argument('--finetune_epochs', type=int, default=100,\n",
        "                        help='number of finetune epochs (default: 100)')\n",
        "    parser.add_argument('--num_folds', type=int, default=10,\n",
        "                        help='number of folds (default: 10)')\n",
        "    parser.add_argument('--lr', type=float, default=0.01,\n",
        "                        help='learning rate (default: 0.01)')\n",
        "    parser.add_argument('--num_cluster', type=int, default=2,\n",
        "                        help='number of clusters (default: 2)')\n",
        "    parser.add_argument('--recluster_interval', type=int, default=20,\n",
        "                        help='the interval of reclustering (default: 20)')\n",
        "    parser.add_argument('--final_dropout', type=float, default=0.5,\n",
        "                        help='final layer dropout (default: 0.5)')\n",
        "    parser.add_argument('--neighbor_pooling_type', type=str, default=\"sum\", choices=[\"sum\", \"average\"],\n",
        "                        help='Pooling for over neighboring nodes: sum or average')\n",
        "    parser.add_argument('--training_scheme', type=str, default=\"decoupled\", choices=[\"decoupled\", \"joint\"],\n",
        "                        help='Training schemes: decoupled or joint')\n",
        "    args = parser.parse_args(args={})\n",
        "\n",
        "    setup_seed(0)\n",
        "    \n",
        "    device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "    # data loading\n",
        "    edge_index, feats, split_idx, label, nb_nodes = load_data(args.dataset, args.num_folds)\n",
        "    input_dim = feats.shape[1]\n",
        "    # pre-clustering and store userID in each clusters\n",
        "    kmeans = KMeans(n_clusters=args.num_cluster, random_state=0).fit(feats)\n",
        "    ss_label = kmeans.labels_\n",
        "    cluster_info = [list(np.where(ss_label==i)[0]) for i in range(args.num_cluster)]\n",
        "    # the shuffled features are used to contruct the negative sample-pairs\n",
        "    idx = np.random.permutation(nb_nodes)\n",
        "    shuf_feats = feats[idx, :]\n",
        "\n",
        "    adj = preprocess_neighbors_sumavepool(torch.LongTensor(edge_index), nb_nodes, device)\n",
        "    feats = torch.FloatTensor(feats).to(device)\n",
        "    shuf_feats = torch.FloatTensor(shuf_feats).to(device)\n",
        "\n",
        "    # pre-training process\n",
        "    model_pretrain = DCI(args.num_layers, args.num_mlp_layers, input_dim, args.hidden_dim, args.neighbor_pooling_type, device).to(device)\n",
        "    if args.training_scheme == 'decoupled':\n",
        "        optimizer_train = optim.Adam(model_pretrain.parameters(), lr=args.lr)\n",
        "        for epoch in range(1, args.epochs + 1):\n",
        "            model_pretrain.train()\n",
        "            loss_pretrain = model_pretrain(feats, shuf_feats, adj, None, None, None, cluster_info, args.num_cluster)\n",
        "            if optimizer_train is not None:\n",
        "                optimizer_train.zero_grad()\n",
        "                loss_pretrain.backward()         \n",
        "                optimizer_train.step()\n",
        "            # re-clustering\n",
        "            if epoch % args.recluster_interval == 0 and epoch < args.epochs:\n",
        "                model_pretrain.eval()\n",
        "                emb = model_pretrain.get_emb(feats, adj)\n",
        "                kmeans = KMeans(n_clusters=args.num_cluster, random_state=0).fit(emb.detach().cpu().numpy())\n",
        "                ss_label = kmeans.labels_\n",
        "                cluster_info = [list(np.where(ss_label==i)[0]) for i in range(args.num_cluster)]\n",
        "        \n",
        "        print('Pre-training Down!')\n",
        "            \n",
        "    #fine-tuning process\n",
        "    fold_idx = 1\n",
        "    every_fold_auc = []\n",
        "    for (train_idx, test_idx) in split_idx:\n",
        "        test_graph = (feats, adj, train_idx, test_idx, label)\n",
        "        tmp_auc = finetune(args, model_pretrain, device, test_graph, input_dim)\n",
        "        every_fold_auc.append(tmp_auc)\n",
        "        print('AUC on the Fold'+str(fold_idx)+': ', tmp_auc)\n",
        "        fold_idx += 1\n",
        "    print('The averaged AUC score: ', np.mean(every_fold_auc))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIKQn7MTxI_d",
        "outputId": "6eb025f0-42fd-4452-8f8f-aa98e7274638"
      },
      "source": [
        "main_dci()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load the edge_index done!\n",
            "Ratio of fraudsters:  0.026376564969004496\n",
            "Number of edges:  18257\n",
            "Number of users:  8227\n",
            "Number of objects:  1000\n",
            "Number of nodes:  9227\n",
            "Pre-training Down!\n",
            "AUC on the Fold1:  0.7190727499716263\n",
            "AUC on the Fold2:  0.7702303938258994\n",
            "AUC on the Fold3:  0.7293156281920328\n",
            "AUC on the Fold4:  0.8036261491317671\n",
            "AUC on the Fold5:  0.7572352740892067\n",
            "AUC on the Fold6:  0.7738622176824423\n",
            "AUC on the Fold7:  0.7627113834979004\n",
            "AUC on the Fold8:  0.645383746507342\n",
            "AUC on the Fold9:  0.6377444860590928\n",
            "AUC on the Fold10:  0.7475179834730397\n",
            "The averaged AUC score:  0.734670001243035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEupCHyd1WSo"
      },
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import random\n",
        "\n",
        "sig = torch.nn.Sigmoid()\n",
        "\n",
        "def setup_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True  \n",
        "\n",
        "def preprocess_neighbors_sumavepool(edge_index, nb_nodes, device):\n",
        "    adj_idx = edge_index\n",
        "        \n",
        "    adj_idx_2 = torch.cat([torch.unsqueeze(adj_idx[1], 0), torch.unsqueeze(adj_idx[0], 0)], 0)\n",
        "    adj_idx = torch.cat([adj_idx, adj_idx_2], 1)\n",
        "\n",
        "    self_loop_edge = torch.LongTensor([range(nb_nodes), range(nb_nodes)])\n",
        "    adj_idx = torch.cat([adj_idx, self_loop_edge], 1)\n",
        "        \n",
        "    adj_elem = torch.ones(adj_idx.shape[1])\n",
        "\n",
        "    adj = torch.sparse.FloatTensor(adj_idx, adj_elem, torch.Size([nb_nodes, nb_nodes]))\n",
        "\n",
        "    return adj.to(device)\n",
        "\n",
        "def evaluate(model, test_graph):\n",
        "    output = model(test_graph[0], test_graph[1])\n",
        "    pred = sig(output.detach().cpu())\n",
        "    test_idx = test_graph[3]\n",
        "    \n",
        "    labels = test_graph[-1]\n",
        "    pred = pred[labels[test_idx, 0].astype('int')].numpy()\n",
        "    target = labels[test_idx, 1]\n",
        "    \n",
        "    false_positive_rate, true_positive_rate, _ = metrics.roc_curve(target, pred, pos_label=1)\n",
        "    auc = metrics.auc(false_positive_rate, true_positive_rate)\n",
        "\n",
        "    return auc\n",
        "\n",
        "def finetune(args, model_pretrain, device, test_graph, feats_num):\n",
        "    # initialize the joint model\n",
        "    model = Classifier(args.num_layers, args.num_mlp_layers, feats_num, args.hidden_dim, args.final_dropout, args.neighbor_pooling_type, device).to(device)\n",
        "    \n",
        "    # replace the encoder in joint model with the pre-trained encoder\n",
        "    pretrained_dict = model_pretrain.state_dict()\n",
        "    model_dict = model.state_dict()\n",
        "    pretrained_dict =  {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "    model_dict.update(pretrained_dict)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "    \n",
        "    criterion_tune = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    res = []\n",
        "    train_idx = test_graph[2]\n",
        "    node_train = test_graph[-1][train_idx, 0].astype('int')\n",
        "    label_train = torch.FloatTensor(test_graph[-1][train_idx, 1]).to(device)\n",
        "    for _ in range(1, args.finetune_epochs+1):\n",
        "        model.train()\n",
        "        output = model(test_graph[0], test_graph[1])\n",
        "        loss = criterion_tune(output[node_train], torch.reshape(label_train, (-1, 1)))\n",
        "        \n",
        "        #backprop\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        # testing\n",
        "        model.eval()\n",
        "        auc = evaluate(model, test_graph)\n",
        "        res.append(auc)\n",
        "\n",
        "    return np.max(res)\n",
        "\n",
        "\n",
        "def main_dgi():\n",
        "    parser = argparse.ArgumentParser(description='PyTorch graph convolutional neurasl net')\n",
        "    parser.add_argument('--dataset', type=str, default=\"wiki\",\n",
        "                        help='name of dataset (default: wiki)')\n",
        "    parser.add_argument('--device', type=int, default=0,\n",
        "                        help='which gpu to use if any (default: 0)')\n",
        "    parser.add_argument('--epochs', type=int, default=50,\n",
        "                        help='number of epochs to train (default: 50)')\n",
        "    parser.add_argument('--finetune_epochs', type=int, default=100,\n",
        "                        help='number of finetune epochs (default: 100)')\n",
        "    parser.add_argument('--num_layers', type=int, default=2,\n",
        "                        help='number of layers (default: 2)')\n",
        "    parser.add_argument('--num_mlp_layers', type=int, default=2,\n",
        "                        help='number of layers for MLP EXCLUDING the input one (default: 2). 1 means linear model.')\n",
        "    parser.add_argument('--hidden_dim', type=int, default=128,\n",
        "                        help='number of hidden units (default: 128)')\n",
        "    parser.add_argument('--num_folds', type=int, default=10,\n",
        "                        help='number of folds (default: 10)')\n",
        "    parser.add_argument('--lr', type=float, default=0.01,\n",
        "                        help='learning rate (default: 0.01)')\n",
        "    parser.add_argument('--final_dropout', type=float, default=0.5,\n",
        "                        help='final layer dropout (default: 0.5)')\n",
        "    parser.add_argument('--neighbor_pooling_type', type=str, default=\"sum\", choices=[\"sum\", \"average\"],\n",
        "                        help='Pooling for over neighboring nodes: sum or average')\n",
        "    args = parser.parse_args(args={})\n",
        "\n",
        "    setup_seed(0)\n",
        "    \n",
        "    device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    # Data loading\n",
        "    edge_index, feats, split_idx, label, nb_nodes = load_data(args.dataset, args.num_folds)\n",
        "    input_dim = feats.shape[1]\n",
        "    # the shuffled features are used to contruct the negative samples\n",
        "    idx = np.random.permutation(nb_nodes)\n",
        "    shuf_feats = feats[idx, :]\n",
        "\n",
        "    model_pretrain = DGI(args.num_layers, args.num_mlp_layers, input_dim, args.hidden_dim, args.neighbor_pooling_type, device).to(device)\n",
        "    optimizer_train = optim.Adam(model_pretrain.parameters(), lr=args.lr)\n",
        "\n",
        "    batch_size = 1\n",
        "    lbl_1 = torch.ones(batch_size, nb_nodes)\n",
        "    lbl_2 = torch.zeros(batch_size, nb_nodes)\n",
        "    lbl = torch.cat((lbl_1, lbl_2), 1).to(device)\n",
        "\n",
        "    adj = preprocess_neighbors_sumavepool(torch.LongTensor(edge_index), nb_nodes, device)\n",
        "    feats = torch.FloatTensor(feats).to(device)\n",
        "    shuf_feats = torch.FloatTensor(shuf_feats).to(device)\n",
        "    \n",
        "    # pre-training\n",
        "    model_pretrain.train()\n",
        "    for _ in range(1, args.epochs + 1):\n",
        "        loss_pretrain = model_pretrain(feats, shuf_feats, adj, None, None, None, lbl)\n",
        "        if optimizer_train is not None:\n",
        "            optimizer_train.zero_grad()\n",
        "            loss_pretrain.backward()         \n",
        "            optimizer_train.step()\n",
        "    \n",
        "    print('Pre-training done!')\n",
        "\n",
        "    #fine-tuning process\n",
        "    fold_idx = 1\n",
        "    every_fold_auc = []\n",
        "    for (train_idx, test_idx) in split_idx:\n",
        "        test_graph = (feats, adj, train_idx, test_idx, label)\n",
        "        tmp_auc = finetune(args, model_pretrain, device, test_graph, input_dim)\n",
        "        every_fold_auc.append(tmp_auc)\n",
        "        print('AUC on the Fold'+str(fold_idx)+': ', tmp_auc)\n",
        "        fold_idx += 1\n",
        "    \n",
        "    print('The averaged AUC score: ', np.mean(every_fold_auc))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lvHmlgd2USV",
        "outputId": "5ce6dcbe-cb5f-4637-e8d2-16711b7d5fad"
      },
      "source": [
        "main_dgi()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load the edge_index done!\n",
            "Ratio of fraudsters:  0.026376564969004496\n",
            "Number of edges:  18257\n",
            "Number of users:  8227\n",
            "Number of objects:  1000\n",
            "Number of nodes:  9227\n",
            "Pre-training done!\n",
            "AUC on the Fold1:  0.7248042219952332\n",
            "AUC on the Fold2:  0.7715355805243447\n",
            "AUC on the Fold3:  0.752581999773011\n",
            "AUC on the Fold4:  0.819572125751901\n",
            "AUC on the Fold5:  0.7289751447054817\n",
            "AUC on the Fold6:  0.7570082850981728\n",
            "AUC on the Fold7:  0.7896663261831801\n",
            "AUC on the Fold8:  0.7332501040366208\n",
            "AUC on the Fold9:  0.6724629926877118\n",
            "AUC on the Fold10:  0.8043517032281078\n",
            "The averaged AUC score:  0.7554208483983766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJV-u_Iy2VUx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}