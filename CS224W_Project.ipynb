{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS224W_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOZ8rW20slv4bWpwMkztclj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjiang9/cs224w-project/blob/main/CS224W_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiGBYOrXMBxM"
      },
      "source": [
        "# **GNNs for Detecting Anomalous Users in Wikipedia with Decoupling Representation Learning and Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnYvFlMNTKxV"
      },
      "source": [
        "In this project, our goal is to detect anomalous users in Wikipedia using methods from the paper \"[Decoupling Representation Learning and Classification for GNN-based Anomaly Detection](https://xiaojingzi.github.io/publications/SIGIR21-Wang-et-al-decoupled-GNN.pdf)\". Most of the code and data are borrowed or adapted from the paper (source: https://github.com/wyl7/DCI-pytorch), though the GIN has been rewritten by us using the PyG framework because it was not originally in PyG - our implementation borrows heavily from [this GNN tutorial](https://github.com/sw-gong/GNN-Tutorial). One significant challenge in anomaly detection is the inconsistency between nodes' behavior patterns and label semantics. This paper seeks to alleviate the issue by decoupling the learning of nodes' embeddings from the learning of classifying their labels, rather than learning both in joint training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZnnFu1kSXlu",
        "outputId": "51ba6204-96d6-4aed-fcc7-ad02efb7e9c1"
      },
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install torch-geometric"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.7/dist-packages (2.0.9)\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.7/dist-packages (0.6.12)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.19.5)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.7/dist-packages (2.0.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.6.3)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.62.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: yacs in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.1.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.13)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.1)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (0.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (57.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFOUIiNEVTq6"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import sklearn.preprocessing as preprocessing\n",
        "from scipy.sparse import linalg\n",
        "import scipy.sparse as sp\n",
        "import sys\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch.nn import Linear, Sequential, ReLU, BatchNorm1d as BN\n",
        "import torch.optim as optim\n",
        "from sklearn import metrics\n",
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFF01qXvUHGU"
      },
      "source": [
        "First, we load and pre-process the dataset (wiki.txt and wiki_label.txt, as found in the github linked above), a Wikipedia editor-page graph where nodes are users or Wiki pages, and edges denote the pages that a user has edited. The dataset contains public ground truth labels of banned users - the task is to identify these anomalous users."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9RH6CiYarx5"
      },
      "source": [
        "# constants\n",
        "EPOCHS = 50\n",
        "N_LAYERS = 3\n",
        "HIDDEN_DIM = 128\n",
        "FINETUNE_EPOCHS = 100\n",
        "N_FOLDS = 10\n",
        "LR = 0.01\n",
        "N_CLUSTERS = 2\n",
        "RECLUSTER_INTERVAL = 20\n",
        "DROPOUT = 0.5"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQdX-j6CUMWE"
      },
      "source": [
        "def load_data(datasets, num_folds):\n",
        "    # load the adjacency\n",
        "    adj = np.loadtxt('./wiki.txt')\n",
        "    num_user = len(set(adj[:, 0]))\n",
        "    num_object = len(set(adj[:, 1]))\n",
        "    adj = adj.astype('int')\n",
        "    nb_nodes = np.max(adj) + 1\n",
        "    edge_index = adj.T\n",
        "    print('Load the edge_index done!')\n",
        "    \n",
        "    # load the user label\n",
        "    label = np.loadtxt('./wiki_label.txt')\n",
        "    y = label[:, 1]\n",
        "    print('Ratio of fraudsters: ', np.sum(y) / len(y))\n",
        "    print('Number of edges: ', edge_index.shape[1])\n",
        "    print('Number of users: ', num_user)\n",
        "    print('Number of objects: ', num_object)\n",
        "    print('Number of nodes: ', nb_nodes)\n",
        "\n",
        "    # split the train_set and validation_set\n",
        "    split_idx = []\n",
        "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=0)\n",
        "    for (train_idx, test_idx) in skf.split(y, y):\n",
        "        split_idx.append((train_idx, test_idx))\n",
        "   \n",
        "    # load initial features\n",
        "    feats = np.load('./wiki_feature64.npy')\n",
        "\n",
        "    return edge_index, feats, split_idx, label, nb_nodes"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s3E7sX-auDM"
      },
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "def eigen_decomposision(n, k, laplacian, hidden_size, retry):\n",
        "    laplacian = laplacian.astype(\"float64\")\n",
        "    ncv = min(n, max(2 * k + 1, 20))\n",
        "    v0 = np.random.rand(n).astype(\"float64\")\n",
        "    for i in range(retry):\n",
        "        try:\n",
        "            s, u = linalg.eigsh(laplacian, k=k, which=\"LA\", ncv=ncv, v0=v0)\n",
        "           \n",
        "        except sparse.linalg.eigen.arpack.ArpackError:\n",
        "            ncv = min(ncv * 2, n)\n",
        "            if i + 1 == retry:\n",
        "                sparse.save_npz(\"arpack_error_sparse_matrix.npz\", laplacian)\n",
        "                u = torch.zeros(n, k)\n",
        "        else:\n",
        "            break\n",
        "    x = preprocessing.normalize(u, norm=\"l2\")\n",
        "    x = x.astype(\"float64\")\n",
        "    return x\n",
        "\n",
        "def intial_embedding(n, adj, in_degree,hidden_size, retry=10):\n",
        "    in_degree = in_degree.clip(1) ** -0.5\n",
        "    norm = sp.diags(in_degree, 0, dtype=float)\n",
        "    laplacian = norm * adj * norm\n",
        "    k = min(n - 2, hidden_size)\n",
        "    x = eigen_decomposision(n, k, laplacian, hidden_size, retry)\n",
        "    \n",
        "    return x\n",
        "\n",
        "def process_adj(dataSetName):\n",
        "    edges = np.loadtxt(dataSetName).astype('int')\n",
        "    node_num = len(set(edges[:, 0])) + len(set(edges[:, 1]))\n",
        "\n",
        "    row = list(edges[:, 0].T) + list(edges[:, 1].T)\n",
        "    col = list(edges[:, 1].T) + list(edges[:, 0].T)\n",
        "    data = [1.0 for _ in range(len(row))]\n",
        "    adj = sp.csr_matrix((data, (row, col)), shape=(node_num, node_num))\n",
        "    return adj, node_num\n",
        "    \n",
        "adj, n = process_adj('./wiki.txt')\n",
        "hidden_size = 64\n",
        "in_degree = [np.sum(adj.data[adj.indptr[i]: adj.indptr[i+1]]) for i in range(n)]\n",
        "in_degree = np.array(in_degree)\n",
        "x = intial_embedding(n, adj, in_degree, hidden_size, retry=10)\n",
        "np.save('wiki_feature64.npy', x)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBTXDvlOp-8R"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, n_h):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.f_k = nn.Bilinear(n_h, n_h, 1)\n",
        "\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, nn.Bilinear):\n",
        "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, c, h_pl, h_mi, s_bias1=None, s_bias2=None):\n",
        "        c_x = torch.unsqueeze(c, 1)\n",
        "        c_x = c_x.expand_as(h_pl)\n",
        "\n",
        "        sc_1 = torch.squeeze(self.f_k(h_pl, c_x), 2)\n",
        "        sc_2 = torch.squeeze(self.f_k(h_mi, c_x), 2)\n",
        "\n",
        "        if s_bias1 is not None:\n",
        "            sc_1 += s_bias1\n",
        "        if s_bias2 is not None:\n",
        "            sc_2 += s_bias2\n",
        "\n",
        "        logits = torch.cat((sc_1, sc_2), 1)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHyWkRU6qL7y"
      },
      "source": [
        "class AvgReadout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AvgReadout, self).__init__()\n",
        "\n",
        "    def forward(self, seq, msk):\n",
        "        if msk is None:\n",
        "            return torch.mean(seq, 1)\n",
        "        else:\n",
        "            msk = torch.unsqueeze(msk, -1)\n",
        "            return torch.sum(seq * msk, 1) / torch.sum(msk)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNjjEvj8SAiq"
      },
      "source": [
        "def reset(nn):\n",
        "    def _reset(item):\n",
        "        if hasattr(item, 'reset_parameters'):\n",
        "            item.reset_parameters()\n",
        "\n",
        "    if nn is not None:\n",
        "        if hasattr(nn, 'children') and len(list(nn.children())) > 0:\n",
        "            for item in nn.children():\n",
        "                _reset(item)\n",
        "        else:\n",
        "            _reset(nn)\n",
        "\n",
        "class GINConv(MessagePassing):\n",
        "    def __init__(self, nn, eps: float = 0., train_eps: bool = False,\n",
        "                 **kwargs):\n",
        "        kwargs.setdefault('aggr', 'add')\n",
        "        super().__init__(**kwargs)\n",
        "        self.nn = nn\n",
        "        self.initial_eps = eps\n",
        "        if train_eps:\n",
        "            self.eps = torch.nn.Parameter(torch.Tensor([eps]))\n",
        "        else:\n",
        "            self.register_buffer('eps', torch.Tensor([eps]))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        reset(self.nn)\n",
        "        self.eps.data.fill_(self.initial_eps)\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index) -> torch.Tensor:\n",
        "        if isinstance(x, torch.Tensor):\n",
        "            x: OptPairTensor = (x, x)\n",
        "\n",
        "        out = self.propagate(edge_index, x=x)\n",
        "\n",
        "        x_r = x[1]\n",
        "        if x_r is not None:\n",
        "            out += (1 + self.eps) * x_r\n",
        "\n",
        "        return self.nn(out)\n",
        "\n",
        "\n",
        "    def message(self, x_j: torch.Tensor) -> torch.Tensor:\n",
        "        return x_j\n",
        "\n",
        "    def message_and_aggregate(self, adj_t, x) -> torch.Tensor:\n",
        "        adj_t = adj_t.set_value(None, layout=None)\n",
        "        return matmul(adj_t, x[0], reduce=self.aggr)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOtvxBGBZPiF"
      },
      "source": [
        "class GIN(torch.nn.Module):\n",
        "    def __init__(self, num_layers, input_dim, hidden):\n",
        "        super(GIN, self).__init__()\n",
        "        self.conv1 = GINConv(Sequential(\n",
        "            Linear(input_dim, hidden),\n",
        "            ReLU(),\n",
        "            Linear(hidden, hidden),\n",
        "            ReLU(),\n",
        "            BN(hidden),\n",
        "        ),\n",
        "                             train_eps=True)\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        for i in range(num_layers - 1):\n",
        "            self.convs.append(\n",
        "                GINConv(Sequential(\n",
        "                    Linear(hidden, hidden),\n",
        "                    ReLU(),\n",
        "                    Linear(hidden, hidden),\n",
        "                    ReLU(),\n",
        "                    BN(hidden),\n",
        "                ),\n",
        "                        train_eps=True))\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.conv1.reset_parameters()\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        edge_index = edge_index._indices()\n",
        "        x = self.conv1(x, edge_index)\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index)\n",
        "        return F.log_softmax(x, dim=-1)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VagOyGhA7Urf"
      },
      "source": [
        "sig = torch.nn.Sigmoid()\n",
        "\n",
        "def setup_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True  \n",
        "\n",
        "def preprocess_neighbors_sumavepool(edge_index, nb_nodes, device):\n",
        "    adj_idx = edge_index\n",
        "        \n",
        "    adj_idx_2 = torch.cat([torch.unsqueeze(adj_idx[1], 0), torch.unsqueeze(adj_idx[0], 0)], 0)\n",
        "    adj_idx = torch.cat([adj_idx, adj_idx_2], 1)\n",
        "\n",
        "    self_loop_edge = torch.LongTensor([range(nb_nodes), range(nb_nodes)])\n",
        "    adj_idx = torch.cat([adj_idx, self_loop_edge], 1)\n",
        "        \n",
        "    adj_elem = torch.ones(adj_idx.shape[1])\n",
        "\n",
        "    adj = torch.sparse.FloatTensor(adj_idx, adj_elem, torch.Size([nb_nodes, nb_nodes]))\n",
        "\n",
        "    return adj.to(device)\n",
        "\n",
        "def evaluate(model, test_graph):\n",
        "    output = model(test_graph[0], test_graph[1])\n",
        "    pred = sig(output.detach().cpu())\n",
        "    test_idx = test_graph[3]\n",
        "    \n",
        "    labels = test_graph[-1]\n",
        "    pred = pred[labels[test_idx, 0].astype('int')].numpy()\n",
        "    target = labels[test_idx, 1]\n",
        "    \n",
        "    false_positive_rate, true_positive_rate, _ = metrics.roc_curve(target, pred, pos_label=1)\n",
        "    auc = metrics.auc(false_positive_rate, true_positive_rate)\n",
        "\n",
        "    return auc\n",
        "\n",
        "def finetune(model_pretrain, device, test_graph, feats_num):\n",
        "    # initialize the joint model\n",
        "    model = Classifier(N_LAYERS, feats_num, HIDDEN_DIM, DROPOUT).to(device)\n",
        "    \n",
        "    # replace the encoder in joint model with the pre-trained encoder\n",
        "    pretrained_dict = model_pretrain.state_dict()\n",
        "    model_dict = model.state_dict()\n",
        "    pretrained_dict =  {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "    model_dict.update(pretrained_dict)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "    \n",
        "    criterion_tune = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    res = []\n",
        "    train_idx = test_graph[2]\n",
        "    node_train = test_graph[-1][train_idx, 0].astype('int')\n",
        "    label_train = torch.FloatTensor(test_graph[-1][train_idx, 1]).to(device)\n",
        "    for _ in range(1, FINETUNE_EPOCHS+1):\n",
        "        model.train()\n",
        "        output = model(test_graph[0], test_graph[1])\n",
        "        loss = criterion_tune(output[node_train], torch.reshape(label_train, (-1, 1)))\n",
        "        \n",
        "        #backprop\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        # testing\n",
        "        model.eval()\n",
        "        auc = evaluate(model, test_graph)\n",
        "        res.append(auc)\n",
        "\n",
        "    return np.max(res)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-wfoTV6pvgf"
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, num_layers, input_dim, hidden_dim, final_dropout):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.gin = GIN(num_layers, input_dim, hidden_dim)\n",
        "        self.linear_prediction = nn.Linear(hidden_dim, 1)\n",
        "        self.final_dropout = final_dropout\n",
        "        \n",
        "    def forward(self, seq1, adj):\n",
        "        h_1 = self.gin(seq1, adj)\n",
        "        score_final_layer = F.dropout(self.linear_prediction(h_1), \n",
        "                                      self.final_dropout, \n",
        "                                      training = self.training)\n",
        "        return score_final_layer"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRyVHimZqXsy"
      },
      "source": [
        "class DCI(nn.Module):\n",
        "    def __init__(self, num_layers, input_dim, hidden_dim):\n",
        "        super(DCI, self).__init__()\n",
        "        self.gin = GIN(num_layers, input_dim, hidden_dim)\n",
        "        self.read = AvgReadout()\n",
        "        self.sigm = nn.Sigmoid()\n",
        "        self.disc = Discriminator(hidden_dim)\n",
        "\n",
        "    def forward(self, seq1, seq2, adj, msk, samp_bias1, samp_bias2, cluster_info, cluster_num):\n",
        "        h_1 = self.gin(seq1, adj)\n",
        "        h_2 = self.gin(seq2, adj)\n",
        "\n",
        "        loss = 0\n",
        "        batch_size = 1\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        for i in range(cluster_num):\n",
        "            node_idx = cluster_info[i]\n",
        "\n",
        "            h_1_block = torch.unsqueeze(h_1[node_idx], 0)\n",
        "            c_block = self.read(h_1_block, msk)\n",
        "            c_block = self.sigm(c_block)\n",
        "            h_2_block = torch.unsqueeze(h_2[node_idx], 0)\n",
        "\n",
        "            lbl_1 = torch.ones(batch_size, len(node_idx))\n",
        "            lbl_2 = torch.zeros(batch_size, len(node_idx))\n",
        "            lbl = torch.cat((lbl_1, lbl_2), 1)\n",
        "\n",
        "            ret = self.disc(c_block, h_1_block, h_2_block, samp_bias1, samp_bias2)\n",
        "            loss_tmp = criterion(ret, lbl)\n",
        "            loss += loss_tmp\n",
        "\n",
        "        return loss / cluster_num\n",
        "    \n",
        "    def get_emb(self, seq1, adj):\n",
        "        h_1 = self.gin(seq1, adj)\n",
        "        return h_1"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEOQudnVqccj"
      },
      "source": [
        "class DGI(nn.Module):\n",
        "    def __init__(self, num_layers, input_dim, hidden_dim):\n",
        "        super(DGI, self).__init__()\n",
        "        self.gin = GIN(num_layers, input_dim, hidden_dim)\n",
        "        self.read = AvgReadout()\n",
        "        self.sigm = nn.Sigmoid()\n",
        "        self.disc = Discriminator(hidden_dim)\n",
        "\n",
        "    def forward(self, seq1, seq2, adj, msk, samp_bias1, samp_bias2, lbl):\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        h_1 = torch.unsqueeze(self.gin(seq1, adj), 0)\n",
        "\n",
        "        c = self.read(h_1, msk)\n",
        "        c = self.sigm(c)\n",
        "\n",
        "        h_2 = torch.unsqueeze(self.gin(seq2, adj), 0)\n",
        "        \n",
        "        ret = self.disc(c, h_1, h_2, samp_bias1, samp_bias2)\n",
        "\n",
        "        loss = criterion(ret, lbl)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_JjoPVkwswc"
      },
      "source": [
        "def main_dci(training_scheme):\n",
        "    setup_seed(0)\n",
        "    \n",
        "    device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "    # data loading\n",
        "    edge_index, feats, split_idx, label, nb_nodes = load_data(\"wiki\", N_FOLDS)\n",
        "    input_dim = feats.shape[1]\n",
        "    # pre-clustering and store userID in each clusters\n",
        "    kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=0).fit(feats)\n",
        "    ss_label = kmeans.labels_\n",
        "    cluster_info = [list(np.where(ss_label==i)[0]) for i in range(N_CLUSTERS)]\n",
        "    # the shuffled features are used to contruct the negative sample-pairs\n",
        "    idx = np.random.permutation(nb_nodes)\n",
        "    shuf_feats = feats[idx, :]\n",
        "\n",
        "    adj = preprocess_neighbors_sumavepool(torch.LongTensor(edge_index), nb_nodes, device)\n",
        "    feats = torch.FloatTensor(feats).to(device)\n",
        "    shuf_feats = torch.FloatTensor(shuf_feats).to(device)\n",
        "\n",
        "    # pre-training process\n",
        "    model_pretrain = DCI(N_LAYERS, input_dim, HIDDEN_DIM).to(device)\n",
        "    if training_scheme == 'decoupled':\n",
        "        optimizer_train = optim.Adam(model_pretrain.parameters(), lr=LR)\n",
        "        for epoch in range(1, EPOCHS + 1):\n",
        "            model_pretrain.train()\n",
        "            loss_pretrain = model_pretrain(feats, shuf_feats, adj, None, None, None, cluster_info, N_CLUSTERS)\n",
        "            if optimizer_train is not None:\n",
        "                optimizer_train.zero_grad()\n",
        "                loss_pretrain.backward()         \n",
        "                optimizer_train.step()\n",
        "            # re-clustering\n",
        "            if epoch % RECLUSTER_INTERVAL == 0 and epoch < EPOCHS:\n",
        "                model_pretrain.eval()\n",
        "                emb = model_pretrain.get_emb(feats, adj)\n",
        "                kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=0).fit(emb.detach().cpu().numpy())\n",
        "                ss_label = kmeans.labels_\n",
        "                cluster_info = [list(np.where(ss_label==i)[0]) for i in range(N_CLUSTERS)]\n",
        "        \n",
        "        print('Pre-training done!')\n",
        "            \n",
        "    #fine-tuning process\n",
        "    fold_idx = 1\n",
        "    every_fold_auc = []\n",
        "    for (train_idx, test_idx) in split_idx:\n",
        "        test_graph = (feats, adj, train_idx, test_idx, label)\n",
        "        tmp_auc = finetune(model_pretrain, device, test_graph, input_dim)\n",
        "        every_fold_auc.append(tmp_auc)\n",
        "        print('AUC on the Fold'+str(fold_idx)+': ', tmp_auc)\n",
        "        fold_idx += 1\n",
        "    print('The averaged AUC score: ', np.mean(every_fold_auc))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIKQn7MTxI_d",
        "outputId": "d624e4cd-fcdf-4844-8d3e-fcff1576ce89"
      },
      "source": [
        "main_dci('decoupled')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load the edge_index done!\n",
            "Ratio of fraudsters:  0.026376564969004496\n",
            "Number of edges:  18257\n",
            "Number of users:  8227\n",
            "Number of objects:  1000\n",
            "Number of nodes:  9227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nWqOLmIbMdB"
      },
      "source": [
        "main_dci('joint')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEupCHyd1WSo"
      },
      "source": [
        "def main_dgi():\n",
        "    setup_seed(0)\n",
        "    \n",
        "    device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    # Data loading\n",
        "    edge_index, feats, split_idx, label, nb_nodes = load_data(\"wiki\", N_FOLDS)\n",
        "    input_dim = feats.shape[1]\n",
        "    # the shuffled features are used to contruct the negative samples\n",
        "    idx = np.random.permutation(nb_nodes)\n",
        "    shuf_feats = feats[idx, :]\n",
        "\n",
        "    model_pretrain = DGI(N_LAYERS, input_dim, HIDDEN_DIM).to(device)\n",
        "    optimizer_train = optim.Adam(model_pretrain.parameters(), lr=LR)\n",
        "\n",
        "    batch_size = 1\n",
        "    lbl_1 = torch.ones(batch_size, nb_nodes)\n",
        "    lbl_2 = torch.zeros(batch_size, nb_nodes)\n",
        "    lbl = torch.cat((lbl_1, lbl_2), 1).to(device)\n",
        "\n",
        "    adj = preprocess_neighbors_sumavepool(torch.LongTensor(edge_index), nb_nodes, device)\n",
        "    feats = torch.FloatTensor(feats).to(device)\n",
        "    shuf_feats = torch.FloatTensor(shuf_feats).to(device)\n",
        "    \n",
        "    # pre-training\n",
        "    model_pretrain.train()\n",
        "    for _ in range(1, EPOCHS + 1):\n",
        "        loss_pretrain = model_pretrain(feats, shuf_feats, adj, None, None, None, lbl)\n",
        "        if optimizer_train is not None:\n",
        "            optimizer_train.zero_grad()\n",
        "            loss_pretrain.backward()         \n",
        "            optimizer_train.step()\n",
        "    \n",
        "    print('Pre-training done!')\n",
        "\n",
        "    #fine-tuning process\n",
        "    fold_idx = 1\n",
        "    every_fold_auc = []\n",
        "    for (train_idx, test_idx) in split_idx:\n",
        "        test_graph = (feats, adj, train_idx, test_idx, label)\n",
        "        tmp_auc = finetune(model_pretrain, device, test_graph, input_dim)\n",
        "        every_fold_auc.append(tmp_auc)\n",
        "        print('AUC on the Fold'+str(fold_idx)+': ', tmp_auc)\n",
        "        fold_idx += 1\n",
        "    \n",
        "    print('The averaged AUC score: ', np.mean(every_fold_auc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lvHmlgd2USV"
      },
      "source": [
        "main_dgi()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4fl7g6qYfqk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}