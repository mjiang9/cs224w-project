{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjiang9/cs224w-project/blob/main/CS224W_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZnnFu1kSXlu",
        "outputId": "cacf35ef-afa5-47eb-de52-dc86a30bb21a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 3.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_sparse-0.6.12-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.19.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.12\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.0.2.tar.gz (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 8.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.62.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.6.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.1.5)\n",
            "Collecting rdflib\n",
            "  Downloading rdflib-6.0.2-py3-none-any.whl (407 kB)\n",
            "\u001b[K     |████████████████████████████████| 407 kB 39.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.6)\n",
            "Collecting yacs\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.13)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Collecting isodate\n",
            "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (57.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.0.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.2-py3-none-any.whl size=535570 sha256=eab05b1ce9202a6ccf333b73199c539a82a8cdaf31e3e7f0f76d81468266dcbd\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/08/13/2321517088bb2e95bfd0e45033bb9c923189e5b2078e0be4ef\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: isodate, yacs, rdflib, torch-geometric\n",
            "Successfully installed isodate-0.6.0 rdflib-6.0.2 torch-geometric-2.0.2 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFOUIiNEVTq6"
      },
      "outputs": [],
      "source": [
        "# Load packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, Sequential, ReLU, BatchNorm1d as BN\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import sklearn.preprocessing as preprocessing\n",
        "from sklearn import metrics\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from scipy.sparse import linalg\n",
        "import scipy.sparse as sp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiGBYOrXMBxM"
      },
      "source": [
        "# **GNNs for Detecting Anomalous Users in Wikipedia with Decoupling Representation Learning and Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnYvFlMNTKxV"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this project, our goal is to detect anomalous users in Wikipedia using methods from the paper \"[Decoupling Representation Learning and Classification for GNN-based Anomaly Detection](https://xiaojingzi.github.io/publications/SIGIR21-Wang-et-al-decoupled-GNN.pdf)\". Most of the code and data are borrowed or adapted from the paper ([source](https://github.com/wyl7/DCI-pytorch)), though the GIN has been rewritten by us using the PyG framework because it was not originally in PyG - our implementation borrows heavily from [this GNN tutorial](https://github.com/sw-gong/GNN-Tutorial). One significant challenge in anomaly detection is the inconsistency between nodes' behavior patterns and label semantics. This paper seeks to alleviate the issue by decoupling the learning of nodes' embeddings from the learning of classifying their labels, rather than learning both in joint training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFF01qXvUHGU"
      },
      "source": [
        "First, we load and pre-process the dataset (wiki.txt and wiki_label.txt, as found in the github linked above), a Wikipedia editor-page graph where nodes are users or Wiki pages, and edges denote the pages that a user has edited. The dataset contains public ground truth labels of banned users - the task is to identify these anomalous users.\n",
        "\n",
        "The code below loads wiki.txt, given as an adjacency matrix, prints some statistics about the graph and its labels, and splits the data for K fold validation. The initial features are loaded here, the next code block will describe the preprocessing done to create them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQdX-j6CUMWE"
      },
      "outputs": [],
      "source": [
        "def load_data(datasets, num_folds):\n",
        "    # load the adjacency\n",
        "    adj = np.loadtxt('./wiki.txt')\n",
        "    num_user = len(set(adj[:, 0]))\n",
        "    num_object = len(set(adj[:, 1]))\n",
        "    adj = adj.astype('int')\n",
        "    nb_nodes = np.max(adj) + 1\n",
        "    edge_index = adj.T\n",
        "    print('Load the edge_index done!')\n",
        "    \n",
        "    # load the user label\n",
        "    label = np.loadtxt('./wiki_label.txt')\n",
        "    y = label[:, 1]\n",
        "    print('Ratio of fraudsters: ', np.sum(y) / len(y))\n",
        "    print('Number of edges: ', edge_index.shape[1])\n",
        "    print('Number of users: ', num_user)\n",
        "    print('Number of objects: ', num_object)\n",
        "    print('Number of nodes: ', nb_nodes)\n",
        "\n",
        "    # split the train_set and validation_set\n",
        "    split_idx = []\n",
        "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=0)\n",
        "    for (train_idx, test_idx) in skf.split(y, y):\n",
        "        split_idx.append((train_idx, test_idx))\n",
        "   \n",
        "    # load initial features\n",
        "    feats = np.load('./wiki_feature64.npy')\n",
        "\n",
        "    return edge_index, feats, split_idx, label, nb_nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulp_t31L8iS5"
      },
      "source": [
        "Then, for preprocessing, we perform the eigendecomposition and L2-normalize the adjacency matrix, such that \n",
        "$$D^{-1/2}AD^{-1/2} = UΛU^T,$$\n",
        "where D is the degree matrix. Since the original data set does not include side information, the eigenvectors roughly capture the users’ behavior and position similarities, since they preserve the adjacency information.\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9s3E7sX-auDM"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "def eigen_decomposision(n, k, laplacian, hidden_size, retry):\n",
        "    laplacian = laplacian.astype(\"float64\")\n",
        "    ncv = min(n, max(2 * k + 1, 20))\n",
        "    v0 = np.random.rand(n).astype(\"float64\")\n",
        "    for i in range(retry):\n",
        "        try:\n",
        "            s, u = linalg.eigsh(laplacian, k=k, which=\"LA\", ncv=ncv, v0=v0)\n",
        "           \n",
        "        except sparse.linalg.eigen.arpack.ArpackError:\n",
        "            ncv = min(ncv * 2, n)\n",
        "            if i + 1 == retry:\n",
        "                sparse.save_npz(\"arpack_error_sparse_matrix.npz\", laplacian)\n",
        "                u = torch.zeros(n, k)\n",
        "        else:\n",
        "            break\n",
        "    x = preprocessing.normalize(u, norm=\"l2\")\n",
        "    x = x.astype(\"float64\")\n",
        "    return x\n",
        "\n",
        "def intial_embedding(n, adj, in_degree,hidden_size, retry=10):\n",
        "    in_degree = in_degree.clip(1) ** -0.5\n",
        "    norm = sp.diags(in_degree, 0, dtype=float)\n",
        "    laplacian = norm * adj * norm\n",
        "    k = min(n - 2, hidden_size)\n",
        "    x = eigen_decomposision(n, k, laplacian, hidden_size, retry)\n",
        "    \n",
        "    return x\n",
        "\n",
        "def process_adj(dataSetName):\n",
        "    edges = np.loadtxt(dataSetName).astype('int')\n",
        "    node_num = len(set(edges[:, 0])) + len(set(edges[:, 1]))\n",
        "\n",
        "    row = list(edges[:, 0].T) + list(edges[:, 1].T)\n",
        "    col = list(edges[:, 1].T) + list(edges[:, 0].T)\n",
        "    data = [1.0 for _ in range(len(row))]\n",
        "    adj = sp.csr_matrix((data, (row, col)), shape=(node_num, node_num))\n",
        "    return adj, node_num\n",
        "    \n",
        "adj, n = process_adj('./wiki.txt')\n",
        "hidden_size = 64\n",
        "in_degree = [np.sum(adj.data[adj.indptr[i]: adj.indptr[i+1]]) for i in range(n)]\n",
        "in_degree = np.array(in_degree)\n",
        "x = intial_embedding(n, adj, in_degree, hidden_size, retry=10)\n",
        "np.save('wiki_feature64.npy', x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpBHUlNPJmST"
      },
      "source": [
        "Here we define some other related function. \n",
        "1. The **Discriminator** is a function that outputs the affinity scores. \n",
        "2. The **AvgReadout** is a average function that applies an average on seq, of shape (batch, nodes, features)\n",
        "\n",
        "These two concepts are introduced in [Deep Graph Infomax (Veličković et al, ICLR 2019)](https://arxiv.org/pdf/1809.10341.pdf) and the code are borrowed from its corresponding [repository](https://github.com/PetarV-/DGI).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBTXDvlOp-8R"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, n_h):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.f_k = nn.Bilinear(n_h, n_h, 1)\n",
        "\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, nn.Bilinear):\n",
        "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, c, h_pl, h_mi, s_bias1=None, s_bias2=None):\n",
        "        c_x = torch.unsqueeze(c, 1)\n",
        "        c_x = c_x.expand_as(h_pl)\n",
        "\n",
        "        sc_1 = torch.squeeze(self.f_k(h_pl, c_x), 2)\n",
        "        sc_2 = torch.squeeze(self.f_k(h_mi, c_x), 2)\n",
        "\n",
        "        if s_bias1 is not None:\n",
        "            sc_1 += s_bias1\n",
        "        if s_bias2 is not None:\n",
        "            sc_2 += s_bias2\n",
        "\n",
        "        logits = torch.cat((sc_1, sc_2), 1)\n",
        "\n",
        "        return logits\n",
        "\n",
        "class AvgReadout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AvgReadout, self).__init__()\n",
        "\n",
        "    def forward(self, seq, msk):\n",
        "        if msk is None:\n",
        "            return torch.mean(seq, 1)\n",
        "        else:\n",
        "            msk = torch.unsqueeze(msk, -1)\n",
        "            return torch.sum(seq * msk, 1) / torch.sum(msk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5BaLLqkJ0mu"
      },
      "source": [
        "# GIN Implementation (PyG)\n",
        "In this section, we will use PyG to implement the GIN, or Graph Isomorphism Network. Gin is a state-of-the-art graph neural network for GNN encoder instantiation. For each node $v$, the $k^{th}$ layer of message processing process is formulated as \n",
        "\n",
        "$$\n",
        "h_{v}^{(k)}=\\operatorname{MLP}^{(k)}\\left(\\left(1+\\epsilon^{(k)}\\right) \\cdot h_{v}^{(k-1)}+\\sum_{u \\in \\mathcal{N}(v)} h_{u}^{(k-1)}\\right)\n",
        "$$\n",
        "\n",
        "Below is the message passing code for GINConv. Note the message and aggregation functions; the message passed is just the feature itself and the aggregation function here is sum ('add'), and the forward function calls propagate, as usual.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNjjEvj8SAiq"
      },
      "outputs": [],
      "source": [
        "def reset(nn):\n",
        "    def _reset(item):\n",
        "        if hasattr(item, 'reset_parameters'):\n",
        "            item.reset_parameters()\n",
        "\n",
        "    if nn is not None:\n",
        "        if hasattr(nn, 'children') and len(list(nn.children())) > 0:\n",
        "            for item in nn.children():\n",
        "                _reset(item)\n",
        "        else:\n",
        "            _reset(nn)\n",
        "\n",
        "class GINConv(MessagePassing):\n",
        "    def __init__(self, nn, eps: float = 0., train_eps: bool = False,\n",
        "                 **kwargs):\n",
        "        kwargs.setdefault('aggr', 'add')\n",
        "        super().__init__(**kwargs)\n",
        "        self.nn = nn\n",
        "        self.initial_eps = eps\n",
        "        if train_eps:\n",
        "            self.eps = torch.nn.Parameter(torch.Tensor([eps]))\n",
        "        else:\n",
        "            self.register_buffer('eps', torch.Tensor([eps]))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        reset(self.nn)\n",
        "        self.eps.data.fill_(self.initial_eps)\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index) -> torch.Tensor:\n",
        "        if isinstance(x, torch.Tensor):\n",
        "            x: OptPairTensor = (x, x)\n",
        "\n",
        "        out = self.propagate(edge_index, x=x)\n",
        "\n",
        "        x_r = x[1]\n",
        "        if x_r is not None:\n",
        "            out += (1 + self.eps) * x_r\n",
        "\n",
        "        return self.nn(out)\n",
        "\n",
        "\n",
        "    def message(self, x_j: torch.Tensor) -> torch.Tensor:\n",
        "        return x_j\n",
        "\n",
        "    def message_and_aggregate(self, adj_t, x) -> torch.Tensor:\n",
        "        adj_t = adj_t.set_value(None, layout=None)\n",
        "        return matmul(adj_t, x[0], reduce=self.aggr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ciyNXjKKzdr"
      },
      "source": [
        "The GIN itself is made up of layers of MLPs, which are linear layers followed by ReLU and batch normalization. GIN uses the GINConv message passing that we implemented above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOtvxBGBZPiF"
      },
      "outputs": [],
      "source": [
        "class GIN(torch.nn.Module):\n",
        "    def __init__(self, num_layers, input_dim, hidden):\n",
        "        super(GIN, self).__init__()\n",
        "        self.conv1 = GINConv(Sequential(\n",
        "            Linear(input_dim, hidden),\n",
        "            ReLU(),\n",
        "            Linear(hidden, hidden),\n",
        "            ReLU(),\n",
        "            BN(hidden),\n",
        "        ),\n",
        "                             train_eps=True)\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        for i in range(num_layers - 1):\n",
        "            self.convs.append(\n",
        "                GINConv(Sequential(\n",
        "                    Linear(hidden, hidden),\n",
        "                    ReLU(),\n",
        "                    Linear(hidden, hidden),\n",
        "                    ReLU(),\n",
        "                    BN(hidden),\n",
        "                ),\n",
        "                        train_eps=True))\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.conv1.reset_parameters()\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        edge_index = edge_index._indices()\n",
        "        x = self.conv1(x, edge_index)\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index)\n",
        "        return F.log_softmax(x, dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPf50qgPM2L-"
      },
      "source": [
        "# Self-supervised Learning (DGI vs DCI)\n",
        "\n",
        "One of the popular self-supervised learning schemes is DGI.  DGI learns the structural similarities of nodes in a global environment by contrasting the whole graph with the node in it. The discriminator outputs the affinity score of each local-global (i.e., node-graph) pair.\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{D G I}=-\\frac{1}{2 n} \\sum_{i=1}^{n}\\left(\\mathbb{E}_{G} \\log \\mathcal{D}\\left(\\boldsymbol{h}_{i}^{(L)}, s\\right)+\\mathbb{E}_{\\tilde{G}} \\log \\left(1-\\mathcal{D}\\left(\\tilde{\\boldsymbol{h}}_{i}^{(L)}, s\\right)\\right)\\right)\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEOQudnVqccj"
      },
      "outputs": [],
      "source": [
        "class DGI(nn.Module):\n",
        "    def __init__(self, num_layers, input_dim, hidden_dim):\n",
        "        super(DGI, self).__init__()\n",
        "        self.gin = GIN(num_layers, input_dim, hidden_dim)\n",
        "        self.read = AvgReadout()\n",
        "        self.sigm = nn.Sigmoid()\n",
        "        self.disc = Discriminator(hidden_dim)\n",
        "\n",
        "    def forward(self, seq1, seq2, adj, msk, samp_bias1, samp_bias2, lbl):\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        h_1 = torch.unsqueeze(self.gin(seq1, adj), 0)\n",
        "\n",
        "        c = self.read(h_1, msk)\n",
        "        c = self.sigm(c)\n",
        "\n",
        "        h_2 = torch.unsqueeze(self.gin(seq2, adj), 0)\n",
        "        \n",
        "        ret = self.disc(c, h_1, h_2, samp_bias1, samp_bias2)\n",
        "\n",
        "        loss = criterion(ret, lbl)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEfKJ9EKLi47"
      },
      "source": [
        "In contrast, DCI can be viewed as a cluster-based DGI. For DCI, the graph is first partitioned into clusters, which account for neighborhood proximity. After clustering, the paper computes the cluster-level representation for each cluster to summarize how the majority in each cluster act, the discriminator outputs the affinity score of each local-semi-global (i.e., node-cluster) pair, and the final loss function is the average of the losses of the clusters.\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{D C I}^{k}=-\\frac{1}{2 n_{k}} \\sum_{v_{i} \\in V_{k}}\\left(\\mathbb{E}_{C_{k}} \\log \\mathcal{D}\\left(\\boldsymbol{h}_{i}, s_{k}\\right)+\\mathbb{E}_{\\tilde{C}_{k}} \\log \\left(1-\\mathcal{D}\\left(\\tilde{\\boldsymbol{h}}_{i}, s_{k}\\right)\\right)\\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRyVHimZqXsy"
      },
      "outputs": [],
      "source": [
        "class DCI(nn.Module):\n",
        "    def __init__(self, num_layers, input_dim, hidden_dim):\n",
        "        super(DCI, self).__init__()\n",
        "        self.gin = GIN(num_layers, input_dim, hidden_dim)\n",
        "        self.read = AvgReadout()\n",
        "        self.sigm = nn.Sigmoid()\n",
        "        self.disc = Discriminator(hidden_dim)\n",
        "\n",
        "    def forward(self, seq1, seq2, adj, msk, samp_bias1, samp_bias2, cluster_info, cluster_num):\n",
        "        h_1 = self.gin(seq1, adj)\n",
        "        h_2 = self.gin(seq2, adj)\n",
        "\n",
        "        loss = 0\n",
        "        batch_size = 1\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        for i in range(cluster_num):\n",
        "            node_idx = cluster_info[i]\n",
        "\n",
        "            h_1_block = torch.unsqueeze(h_1[node_idx], 0)\n",
        "            c_block = self.read(h_1_block, msk)\n",
        "            c_block = self.sigm(c_block)\n",
        "            h_2_block = torch.unsqueeze(h_2[node_idx], 0)\n",
        "\n",
        "            lbl_1 = torch.ones(batch_size, len(node_idx))\n",
        "            lbl_2 = torch.zeros(batch_size, len(node_idx))\n",
        "            lbl = torch.cat((lbl_1, lbl_2), 1)\n",
        "\n",
        "            ret = self.disc(c_block, h_1_block, h_2_block, samp_bias1, samp_bias2)\n",
        "            loss_tmp = criterion(ret, lbl)\n",
        "            loss += loss_tmp\n",
        "\n",
        "        return loss / cluster_num\n",
        "    \n",
        "    def get_emb(self, seq1, adj):\n",
        "        h_1 = self.gin(seq1, adj)\n",
        "        return h_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvvZKUk6MJ66"
      },
      "source": [
        "At the end of this section, we define the classifier. The classifier is simply a GIN with a linear prediction layer, we also use dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-wfoTV6pvgf"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, num_layers, input_dim, hidden_dim, final_dropout):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.gin = GIN(num_layers, input_dim, hidden_dim)\n",
        "        self.linear_prediction = nn.Linear(hidden_dim, 1)\n",
        "        self.final_dropout = final_dropout\n",
        "        \n",
        "    def forward(self, seq1, adj):\n",
        "        h_1 = self.gin(seq1, adj)\n",
        "        score_final_layer = F.dropout(self.linear_prediction(h_1), \n",
        "                                      self.final_dropout, \n",
        "                                      training = self.training)\n",
        "        return score_final_layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXSFK96_MR-F"
      },
      "source": [
        "# Other Helper Functions\n",
        "In this section, we define some helper functions used by the model, to:\n",
        "*   set the seed (setup_seed),\n",
        "*   construct the adjacency matrix handling self loops (preprocess_neighbors_sumavepool),\n",
        "*   evaluate a model on a given graph by generating predictions and comparing to the actual labels using AUC (evaluate), and \n",
        "*  finetune a model for node classification with the Adam optimizer and the classification model we created above (finetune)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VagOyGhA7Urf"
      },
      "outputs": [],
      "source": [
        "sig = torch.nn.Sigmoid()\n",
        "\n",
        "def setup_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True  \n",
        "\n",
        "def preprocess_neighbors_sumavepool(edge_index, nb_nodes, device):\n",
        "    adj_idx = edge_index\n",
        "        \n",
        "    adj_idx_2 = torch.cat([torch.unsqueeze(adj_idx[1], 0), torch.unsqueeze(adj_idx[0], 0)], 0)\n",
        "    adj_idx = torch.cat([adj_idx, adj_idx_2], 1)\n",
        "\n",
        "    self_loop_edge = torch.LongTensor([range(nb_nodes), range(nb_nodes)])\n",
        "    adj_idx = torch.cat([adj_idx, self_loop_edge], 1)\n",
        "        \n",
        "    adj_elem = torch.ones(adj_idx.shape[1])\n",
        "\n",
        "    adj = torch.sparse.FloatTensor(adj_idx, adj_elem, torch.Size([nb_nodes, nb_nodes]))\n",
        "\n",
        "    return adj.to(device)\n",
        "\n",
        "def evaluate(model, test_graph):\n",
        "    output = model(test_graph[0], test_graph[1])\n",
        "    pred = sig(output.detach().cpu())\n",
        "    test_idx = test_graph[3]\n",
        "    \n",
        "    labels = test_graph[-1]\n",
        "    pred = pred[labels[test_idx, 0].astype('int')].numpy()\n",
        "    target = labels[test_idx, 1]\n",
        "    \n",
        "    false_positive_rate, true_positive_rate, _ = metrics.roc_curve(target, pred, pos_label=1)\n",
        "    auc = metrics.auc(false_positive_rate, true_positive_rate)\n",
        "\n",
        "    return auc\n",
        "\n",
        "def finetune(model_pretrain, device, test_graph, feats_num):\n",
        "    # initialize the joint model\n",
        "    model = Classifier(N_LAYERS, feats_num, HIDDEN_DIM, DROPOUT).to(device)\n",
        "    \n",
        "    # replace the encoder in joint model with the pre-trained encoder\n",
        "    pretrained_dict = model_pretrain.state_dict()\n",
        "    model_dict = model.state_dict()\n",
        "    pretrained_dict =  {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "    model_dict.update(pretrained_dict)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "    \n",
        "    criterion_tune = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    res = []\n",
        "    train_idx = test_graph[2]\n",
        "    node_train = test_graph[-1][train_idx, 0].astype('int')\n",
        "    label_train = torch.FloatTensor(test_graph[-1][train_idx, 1]).to(device)\n",
        "    for _ in range(1, FINETUNE_EPOCHS+1):\n",
        "        model.train()\n",
        "        output = model(test_graph[0], test_graph[1])\n",
        "        loss = criterion_tune(output[node_train], torch.reshape(label_train, (-1, 1)))\n",
        "        \n",
        "        #backprop\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        # testing\n",
        "        model.eval()\n",
        "        auc = evaluate(model, test_graph)\n",
        "        res.append(auc)\n",
        "\n",
        "    return np.max(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M5QtENoMTLn"
      },
      "source": [
        "# Model Training and Testing\n",
        "In the last section, we present the main code and results we attained by running DCI with the decoupled method/pretraining and with joint training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9RH6CiYarx5"
      },
      "outputs": [],
      "source": [
        "# constants\n",
        "EPOCHS = 50\n",
        "N_LAYERS = 2\n",
        "HIDDEN_DIM = 128\n",
        "FINETUNE_EPOCHS = 100\n",
        "N_FOLDS = 10\n",
        "LR = 0.01\n",
        "N_CLUSTERS = 2\n",
        "RECLUSTER_INTERVAL = 20\n",
        "DROPOUT = 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model using DCI first loads and preprocesses the data, clusters the data, then runs pre-training in the case of decoupled training which is self-supervised learning based on the cluster groupings, then finetunes for the classification task. For joint training, the self-supervised learning step is skipped, so the node representations are learned jointly with learning to classify the labels. The AUC results are reported below for decoupled and joint training. "
      ],
      "metadata": {
        "id": "5mi2T0Bu3eJj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_JjoPVkwswc"
      },
      "outputs": [],
      "source": [
        "def main_dci(training_scheme):\n",
        "    setup_seed(0)\n",
        "    \n",
        "    device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "    # data loading\n",
        "    edge_index, feats, split_idx, label, nb_nodes = load_data(\"wiki\", N_FOLDS)\n",
        "    input_dim = feats.shape[1]\n",
        "    # pre-clustering and store userID in each clusters\n",
        "    kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=0).fit(feats)\n",
        "    ss_label = kmeans.labels_\n",
        "    cluster_info = [list(np.where(ss_label==i)[0]) for i in range(N_CLUSTERS)]\n",
        "    # the shuffled features are used to contruct the negative sample-pairs\n",
        "    idx = np.random.permutation(nb_nodes)\n",
        "    shuf_feats = feats[idx, :]\n",
        "\n",
        "    adj = preprocess_neighbors_sumavepool(torch.LongTensor(edge_index), nb_nodes, device)\n",
        "    feats = torch.FloatTensor(feats).to(device)\n",
        "    shuf_feats = torch.FloatTensor(shuf_feats).to(device)\n",
        "\n",
        "    # pre-training process\n",
        "    model_pretrain = DCI(N_LAYERS, input_dim, HIDDEN_DIM).to(device)\n",
        "    if training_scheme == 'decoupled':\n",
        "        optimizer_train = optim.Adam(model_pretrain.parameters(), lr=LR)\n",
        "        for epoch in range(1, EPOCHS + 1):\n",
        "            model_pretrain.train()\n",
        "            loss_pretrain = model_pretrain(feats, shuf_feats, adj, None, None, None, cluster_info, N_CLUSTERS)\n",
        "            if optimizer_train is not None:\n",
        "                optimizer_train.zero_grad()\n",
        "                loss_pretrain.backward()         \n",
        "                optimizer_train.step()\n",
        "            # re-clustering\n",
        "            if epoch % RECLUSTER_INTERVAL == 0 and epoch < EPOCHS:\n",
        "                model_pretrain.eval()\n",
        "                emb = model_pretrain.get_emb(feats, adj)\n",
        "                kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=0).fit(emb.detach().cpu().numpy())\n",
        "                ss_label = kmeans.labels_\n",
        "                cluster_info = [list(np.where(ss_label==i)[0]) for i in range(N_CLUSTERS)]\n",
        "        \n",
        "        print('Pre-training done!')\n",
        "            \n",
        "    #fine-tuning process\n",
        "    fold_idx = 1\n",
        "    every_fold_auc = []\n",
        "    for (train_idx, test_idx) in split_idx:\n",
        "        test_graph = (feats, adj, train_idx, test_idx, label)\n",
        "        tmp_auc = finetune(model_pretrain, device, test_graph, input_dim)\n",
        "        every_fold_auc.append(tmp_auc)\n",
        "        print('AUC on the Fold'+str(fold_idx)+': ', tmp_auc)\n",
        "        fold_idx += 1\n",
        "    print('The averaged AUC score: ', np.mean(every_fold_auc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIKQn7MTxI_d",
        "outputId": "86b5774d-9f15-4854-96ec-6b664c673580"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load the edge_index done!\n",
            "Ratio of fraudsters:  0.026376564969004496\n",
            "Number of edges:  18257\n",
            "Number of users:  8227\n",
            "Number of objects:  1000\n",
            "Number of nodes:  9227\n",
            "Pre-training done!\n",
            "AUC on the Fold1:  0.6536431733060947\n",
            "AUC on the Fold2:  0.7239246396549768\n",
            "AUC on the Fold3:  0.6196799455226422\n",
            "AUC on the Fold4:  0.6583531948700488\n",
            "AUC on the Fold5:  0.6761434570423335\n",
            "AUC on the Fold6:  0.7482124616956078\n",
            "AUC on the Fold7:  0.7132277834525026\n",
            "AUC on the Fold8:  0.6179775280898876\n",
            "AUC on the Fold9:  0.6045122168717674\n",
            "AUC on the Fold10:  0.6928244456334344\n",
            "The averaged AUC score:  0.6708498846139295\n"
          ]
        }
      ],
      "source": [
        "main_dci('decoupled')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nWqOLmIbMdB",
        "outputId": "561d8089-bf76-45e1-c792-1b2daad0ddeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load the edge_index done!\n",
            "Ratio of fraudsters:  0.026376564969004496\n",
            "Number of edges:  18257\n",
            "Number of users:  8227\n",
            "Number of objects:  1000\n",
            "Number of nodes:  9227\n",
            "AUC on the Fold1:  0.654494382022472\n",
            "AUC on the Fold2:  0.7449778685733742\n",
            "AUC on the Fold3:  0.63959822948587\n",
            "AUC on the Fold4:  0.7015378504142549\n",
            "AUC on the Fold5:  0.6558846895925547\n",
            "AUC on the Fold6:  0.7219384859834299\n",
            "AUC on the Fold7:  0.7250312109862672\n",
            "AUC on the Fold8:  0.6643481362582486\n",
            "AUC on the Fold9:  0.6187800963081862\n",
            "AUC on the Fold10:  0.66916354556804\n",
            "The averaged AUC score:  0.6795754495192697\n"
          ]
        }
      ],
      "source": [
        "main_dci('joint')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBuSOerWMU8M"
      },
      "source": [
        "Below is the main code and results we attained running DGI with the decoupled method/pretraining. Note that DGI, unlike DCI, does not do the clustering step; it simply loads the data, preprocesses it, pre-trains using the DGI self-supervised learning method from earlier, then fine-tunes for the classification task. The AUC results are reported below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEupCHyd1WSo"
      },
      "outputs": [],
      "source": [
        "def main_dgi():\n",
        "    setup_seed(0)\n",
        "    \n",
        "    device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    # Data loading\n",
        "    edge_index, feats, split_idx, label, nb_nodes = load_data(\"wiki\", N_FOLDS)\n",
        "    input_dim = feats.shape[1]\n",
        "    # the shuffled features are used to contruct the negative samples\n",
        "    idx = np.random.permutation(nb_nodes)\n",
        "    shuf_feats = feats[idx, :]\n",
        "\n",
        "    model_pretrain = DGI(N_LAYERS, input_dim, HIDDEN_DIM).to(device)\n",
        "    optimizer_train = optim.Adam(model_pretrain.parameters(), lr=LR)\n",
        "\n",
        "    batch_size = 1\n",
        "    lbl_1 = torch.ones(batch_size, nb_nodes)\n",
        "    lbl_2 = torch.zeros(batch_size, nb_nodes)\n",
        "    lbl = torch.cat((lbl_1, lbl_2), 1).to(device)\n",
        "\n",
        "    adj = preprocess_neighbors_sumavepool(torch.LongTensor(edge_index), nb_nodes, device)\n",
        "    feats = torch.FloatTensor(feats).to(device)\n",
        "    shuf_feats = torch.FloatTensor(shuf_feats).to(device)\n",
        "    \n",
        "    # pre-training\n",
        "    model_pretrain.train()\n",
        "    for _ in range(1, EPOCHS + 1):\n",
        "        loss_pretrain = model_pretrain(feats, shuf_feats, adj, None, None, None, lbl)\n",
        "        if optimizer_train is not None:\n",
        "            optimizer_train.zero_grad()\n",
        "            loss_pretrain.backward()         \n",
        "            optimizer_train.step()\n",
        "    \n",
        "    print('Pre-training done!')\n",
        "\n",
        "    #fine-tuning process\n",
        "    fold_idx = 1\n",
        "    every_fold_auc = []\n",
        "    for (train_idx, test_idx) in split_idx:\n",
        "        test_graph = (feats, adj, train_idx, test_idx, label)\n",
        "        tmp_auc = finetune(model_pretrain, device, test_graph, input_dim)\n",
        "        every_fold_auc.append(tmp_auc)\n",
        "        print('AUC on the Fold'+str(fold_idx)+': ', tmp_auc)\n",
        "        fold_idx += 1\n",
        "    \n",
        "    print('The averaged AUC score: ', np.mean(every_fold_auc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lvHmlgd2USV",
        "outputId": "3c24c515-96b4-420d-e8fc-60ba53132847"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load the edge_index done!\n",
            "Ratio of fraudsters:  0.026376564969004496\n",
            "Number of edges:  18257\n",
            "Number of users:  8227\n",
            "Number of objects:  1000\n",
            "Number of nodes:  9227\n",
            "Pre-training done!\n",
            "AUC on the Fold1:  0.6918908182953127\n",
            "AUC on the Fold2:  0.6718306662126887\n",
            "AUC on the Fold3:  0.6481103166496426\n",
            "AUC on the Fold4:  0.7107876517988878\n",
            "AUC on the Fold5:  0.6229712858926343\n",
            "AUC on the Fold6:  0.7325502213142663\n",
            "AUC on the Fold7:  0.6872942912268756\n",
            "AUC on the Fold8:  0.6303430236014504\n",
            "AUC on the Fold9:  0.6047500148623743\n",
            "AUC on the Fold10:  0.6730872124130551\n",
            "The averaged AUC score:  0.6673615502267187\n"
          ]
        }
      ],
      "source": [
        "main_dgi()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4fl7g6qYfqk"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CS224W_Project.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}